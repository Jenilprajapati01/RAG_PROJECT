{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWK-qrtRn-v-"
      },
      "source": [
        "## Setting up the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "HVK_hO-EU99g",
        "outputId": "75e81ca3-4f42-4423-ce9b-1f246985cff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.2.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.2.7 (from langchain)\n",
            "  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.5.0)\n",
            "Downloading langchain-1.2.6-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.6\n",
            "    Uninstalling langchain-core-1.2.6:\n",
            "      Successfully uninstalled langchain-core-1.2.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.2.3\n",
            "    Uninstalling langchain-1.2.3:\n",
            "      Successfully uninstalled langchain-1.2.3\n",
            "Successfully installed langchain-1.2.6 langchain-core-1.2.7\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.2.7)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (0.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-genai<2.0.0,>=1.56.0 (from langchain-google-genai)\n",
            "  Downloading google_genai-1.59.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<2.0.0,>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.12.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
            "Collecting google-auth<3.0.0,>=2.47.0 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.13.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_genai-1.59.0-py3-none-any.whl (719 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.1/719.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-auth, google-genai, langchain-google-genai\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.43.0\n",
            "    Uninstalling google-auth-2.43.0:\n",
            "      Successfully uninstalled google-auth-2.43.0\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.55.0\n",
            "    Uninstalling google-genai-1.55.0:\n",
            "      Successfully uninstalled google-genai-1.55.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-auth-2.47.0 google-genai-1.59.0 langchain-google-genai-4.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0d0f1c6ba37e4ad4ac66696bc5aa5d68",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade langchain-core\n",
        "!pip install --upgrade langchain-community\n",
        "!pip install --upgrade langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AULGUVyIFQ70",
        "outputId": "05ef905b-0a95-46ad-f69f-eadf43b337f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class ChatGoogleGenerativeAI in module langchain_google_genai.chat_models:\n",
            "\n",
            "class ChatGoogleGenerativeAI(langchain_google_genai._common._BaseGoogleGenerativeAI, langchain_core.language_models.chat_models.BaseChatModel)\n",
            " |  ChatGoogleGenerativeAI(*, name: str | None = None, cache: langchain_core.caches.BaseCache | bool | None = None, verbose: bool = <factory>, callbacks: list[langchain_core.callbacks.base.BaseCallbackHandler] | langchain_core.callbacks.base.BaseCallbackManager | None = None, tags: list[str] | None = None, metadata: dict[str, typing.Any] | None = None, custom_get_token_ids: collections.abc.Callable[[str], list[int]] | None = None, rate_limiter: langchain_core.rate_limiters.BaseRateLimiter | None = None, disable_streaming: Union[bool, Literal['tool_calling']] = False, output_version: str | None = <factory>, profile: langchain_core.language_models.model_profile.ModelProfile | None = None, api_key: pydantic.types.SecretStr | None = <factory>, credentials: Any = None, vertexai: bool | None = None, project: str | None = None, location: str | None = <factory>, client_options: str | dict | None = None, additional_headers: dict[str, str] | None = None, client_args: dict[str, typing.Any] | None = None, model: str, temperature: float = 0.7, top_p: float | None = None, top_k: int | None = None, max_tokens: int | None = None, n: int = 1, retries: int = 6, request_timeout: float | None = None, response_modalities: list[google.genai.types.Modality] | None = None, media_resolution: google.genai.types.MediaResolution | None = None, image_config: dict[str, typing.Any] | None = None, thinking_budget: int | None = None, include_thoughts: bool | None = None, safety_settings: dict[google.genai.types.HarmCategory, google.genai.types.HarmBlockThreshold] | None = None, seed: int | None = None, labels: dict[str, str] | None = None, client: google.genai.client.Client | None = None, default_metadata_input: collections.abc.Sequence[tuple[str, str]] | None = None, model_kwargs: dict[str, typing.Any] = <factory>, streaming: bool | None = None, convert_system_message_to_human: bool = False, stop: list[str] | None = None, response_mime_type: str | None = None, response_schema: dict[str, typing.Any] | None = None, thinking_level: Optional[Literal['minimal', 'low', 'medium', 'high']] = None, cached_content: str | None = None) -> None\n",
            " |\n",
            " |  Google GenAI chat model integration.\n",
            " |\n",
            " |  Setup:\n",
            " |      !!! version-added \"Vertex AI Platform Support\"\n",
            " |\n",
            " |          Added in `langchain-google-genai` 4.0.0.\n",
            " |\n",
            " |          `ChatGoogleGenerativeAI` now supports both the **Gemini Developer API** and\n",
            " |          **Vertex AI Platform** as backend options.\n",
            " |\n",
            " |      **For Gemini Developer API** (simplest):\n",
            " |\n",
            " |      1. Set the `GOOGLE_API_KEY` environment variable (recommended), or\n",
            " |      2. Pass your API key using the [`api_key`][langchain_google_genai.ChatGoogleGenerativeAI.google_api_key]\n",
            " |          parameter\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\", api_key=\"...\")\n",
            " |      ```\n",
            " |\n",
            " |      **For Vertex AI Platform with API key**:\n",
            " |\n",
            " |      ```bash\n",
            " |      export GEMINI_API_KEY='your-api-key'\n",
            " |      export GOOGLE_GENAI_USE_VERTEXAI=true\n",
            " |      export GOOGLE_CLOUD_PROJECT='your-project-id'\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |      # Or explicitly:\n",
            " |      model = ChatGoogleGenerativeAI(\n",
            " |          model=\"gemini-3-pro-preview\",\n",
            " |          api_key=\"...\",\n",
            " |          project=\"your-project-id\",\n",
            " |          vertexai=True,\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      **For Vertex AI with credentials**:\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(\n",
            " |          model=\"gemini-2.5-flash\",\n",
            " |          project=\"your-project-id\",\n",
            " |          # Uses Application Default Credentials (ADC)\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      **Automatic backend detection** (when `vertexai=None` / unspecified):\n",
            " |\n",
            " |      1. If `GOOGLE_GENAI_USE_VERTEXAI` env var is set, uses that value\n",
            " |      2. If `credentials` parameter is provided, uses Vertex AI\n",
            " |      3. If `project` parameter is provided, uses Vertex AI\n",
            " |      4. Otherwise, uses Gemini Developer API\n",
            " |\n",
            " |  Environment variables:\n",
            " |      | Variable | Purpose | Backend |\n",
            " |      |----------|---------|---------|\n",
            " |      | `GOOGLE_API_KEY` | API key (primary) | Both (see `GOOGLE_GENAI_USE_VERTEXAI`) |\n",
            " |      | `GEMINI_API_KEY` | API key (fallback) | Both (see `GOOGLE_GENAI_USE_VERTEXAI`) |\n",
            " |      | `GOOGLE_GENAI_USE_VERTEXAI` | Force Vertex AI backend (`true`/`false`) | Vertex AI |\n",
            " |      | `GOOGLE_CLOUD_PROJECT` | GCP project ID | Vertex AI |\n",
            " |      | `GOOGLE_CLOUD_LOCATION` | GCP region (default: `us-central1`) | Vertex AI |\n",
            " |      | `HTTPS_PROXY` | HTTP/HTTPS proxy URL | Both |\n",
            " |      | `SSL_CERT_FILE` | Custom SSL certificate file | Both |\n",
            " |\n",
            " |      `GOOGLE_API_KEY` is checked first for backwards compatibility. (`GEMINI_API_KEY`\n",
            " |      was introduced later to better reflect the API's branding.)\n",
            " |\n",
            " |  Proxy configuration:\n",
            " |      Set these before initializing:\n",
            " |\n",
            " |      ```bash\n",
            " |      export HTTPS_PROXY='http://username:password@proxy_uri:port'\n",
            " |      export SSL_CERT_FILE='path/to/cert.pem'  # Optional: custom SSL certificate\n",
            " |      ```\n",
            " |\n",
            " |      For SOCKS5 proxies or advanced proxy configuration, use the\n",
            " |      [`client_args`][langchain_google_genai.ChatGoogleGenerativeAI.client_args]\n",
            " |      parameter:\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(\n",
            " |          model=\"gemini-2.5-flash\",\n",
            " |          client_args={\"proxy\": \"socks5://user:pass@host:port\"},\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Instantiation\"\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |      model.invoke(\"Write me a ballad about LangChain\")\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Invoke\"\n",
            " |\n",
            " |      ```python\n",
            " |      messages = [\n",
            " |          (\"system\", \"Translate the user sentence to French.\"),\n",
            " |          (\"human\", \"I love programming.\"),\n",
            " |      ]\n",
            " |      model.invoke(messages)\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      AIMessage(\n",
            " |          content=[\n",
            " |              {\n",
            " |                  \"type\": \"text\",\n",
            " |                  \"text\": \"**J'adore la programmation.**\\n\\nYou can also say:...\",\n",
            " |                  \"extras\": {\"signature\": \"Eq0W...\"},\n",
            " |              }\n",
            " |          ],\n",
            " |          additional_kwargs={},\n",
            " |          response_metadata={\n",
            " |              \"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []},\n",
            " |              \"finish_reason\": \"STOP\",\n",
            " |              \"model_name\": \"gemini-3-pro-preview\",\n",
            " |              \"safety_ratings\": [],\n",
            " |              \"model_provider\": \"google_genai\",\n",
            " |          },\n",
            " |          id=\"lc_run--63a04ced-6b63-4cf6-86a1-c32fa565938e-0\",\n",
            " |          usage_metadata={\n",
            " |              \"input_tokens\": 12,\n",
            " |              \"output_tokens\": 826,\n",
            " |              \"total_tokens\": 838,\n",
            " |              \"input_token_details\": {\"cache_read\": 0},\n",
            " |              \"output_token_details\": {\"reasoning\": 777},\n",
            " |          },\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      !!! note \"`content` format\"\n",
            " |\n",
            " |          The shape of `content` may differ based on the model chosen. See\n",
            " |          [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#invocation)\n",
            " |          for more info.\n",
            " |\n",
            " |  ???+ example \"Stream\"\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
            " |\n",
            " |      for chunk in model.stream(messages):\n",
            " |          print(chunk)\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      AIMessageChunk(\n",
            " |          content=\"J\",\n",
            " |          response_metadata={\"finish_reason\": \"STOP\", \"safety_ratings\": []},\n",
            " |          id=\"run-e905f4f4-58cb-4a10-a960-448a2bb649e3\",\n",
            " |          usage_metadata={\n",
            " |              \"input_tokens\": 18,\n",
            " |              \"output_tokens\": 1,\n",
            " |              \"total_tokens\": 19,\n",
            " |          },\n",
            " |      )\n",
            " |      AIMessageChunk(\n",
            " |          content=\"'adore programmer. \\\\n\",\n",
            " |          response_metadata={\n",
            " |              \"finish_reason\": \"STOP\",\n",
            " |              \"safety_ratings\": [\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |              ],\n",
            " |          },\n",
            " |          id=\"run-e905f4f4-58cb-4a10-a960-448a2bb649e3\",\n",
            " |          usage_metadata={\n",
            " |              \"input_tokens\": 18,\n",
            " |              \"output_tokens\": 5,\n",
            " |              \"total_tokens\": 23,\n",
            " |          },\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      To assemble a full [`AIMessage`][langchain.messages.AIMessage] message from a\n",
            " |      stream of chunks:\n",
            " |\n",
            " |      ```python\n",
            " |      stream = model.stream(messages)\n",
            " |      full = next(stream)\n",
            " |      for chunk in stream:\n",
            " |          full += chunk\n",
            " |      full\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      AIMessageChunk(\n",
            " |          content=\"J'adore programmer. \\\\n\",\n",
            " |          response_metadata={\n",
            " |              \"finish_reason\": \"STOPSTOP\",\n",
            " |              \"safety_ratings\": [\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |              ],\n",
            " |          },\n",
            " |          id=\"run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec\",\n",
            " |          usage_metadata={\n",
            " |              \"input_tokens\": 36,\n",
            " |              \"output_tokens\": 6,\n",
            " |              \"total_tokens\": 42,\n",
            " |          },\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      !!! note \"`content` format\"\n",
            " |\n",
            " |          The shape of `content` may differ based on the model chosen. See\n",
            " |          [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#invocation)\n",
            " |          for more info.\n",
            " |\n",
            " |  ???+ example \"Async invocation\"\n",
            " |\n",
            " |      ```python\n",
            " |      await model.ainvoke(messages)\n",
            " |\n",
            " |      # stream:\n",
            " |      async for chunk in (await model.astream(messages))\n",
            " |\n",
            " |      # batch:\n",
            " |      await model.abatch([messages])\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Tool calling\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#tool-calling)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      from pydantic import BaseModel, Field\n",
            " |\n",
            " |\n",
            " |      class GetWeather(BaseModel):\n",
            " |          '''Get the current weather in a given location'''\n",
            " |\n",
            " |          location: str = Field(\n",
            " |              ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
            " |          )\n",
            " |\n",
            " |\n",
            " |      class GetPopulation(BaseModel):\n",
            " |          '''Get the current population in a given location'''\n",
            " |\n",
            " |          location: str = Field(\n",
            " |              ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
            " |          )\n",
            " |\n",
            " |\n",
            " |      llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
            " |      ai_msg = llm_with_tools.invoke(\n",
            " |          \"Which city is hotter today and which is bigger: LA or NY?\"\n",
            " |      )\n",
            " |      ai_msg.tool_calls\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      [\n",
            " |          {\n",
            " |              \"name\": \"GetWeather\",\n",
            " |              \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            " |              \"id\": \"c186c99f-f137-4d52-947f-9e3deabba6f6\",\n",
            " |          },\n",
            " |          {\n",
            " |              \"name\": \"GetWeather\",\n",
            " |              \"args\": {\"location\": \"New York City, NY\"},\n",
            " |              \"id\": \"cebd4a5d-e800-4fa5-babd-4aa286af4f31\",\n",
            " |          },\n",
            " |          {\n",
            " |              \"name\": \"GetPopulation\",\n",
            " |              \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            " |              \"id\": \"4f92d897-f5e4-4d34-a3bc-93062c92591e\",\n",
            " |          },\n",
            " |          {\n",
            " |              \"name\": \"GetPopulation\",\n",
            " |              \"args\": {\"location\": \"New York City, NY\"},\n",
            " |              \"id\": \"634582de-5186-4e4b-968b-f192f0a93678\",\n",
            " |          },\n",
            " |      ]\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Structured output\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#structured-output)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      from typing import Optional\n",
            " |\n",
            " |      from pydantic import BaseModel, Field\n",
            " |\n",
            " |\n",
            " |      class Joke(BaseModel):\n",
            " |          '''Joke to tell user.'''\n",
            " |\n",
            " |          setup: str = Field(description=\"The setup of the joke\")\n",
            " |          punchline: str = Field(description=\"The punchline to the joke\")\n",
            " |          rating: Optional[int] = Field(\n",
            " |              description=\"How funny the joke is, from 1 to 10\"\n",
            " |          )\n",
            " |\n",
            " |\n",
            " |      # Default method uses json_schema for reliable structured output\n",
            " |      structured_model = model.with_structured_output(Joke)\n",
            " |      structured_model.invoke(\"Tell me a joke about cats\")\n",
            " |\n",
            " |      # Alternative: use function_calling method (less reliable)\n",
            " |      structured_model_fc = model.with_structured_output(\n",
            " |          Joke, method=\"function_calling\"\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      Joke(\n",
            " |          setup=\"Why are cats so good at video games?\",\n",
            " |          punchline=\"They have nine lives on the internet\",\n",
            " |          rating=None,\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      Two methods are supported for structured output:\n",
            " |\n",
            " |      * `method='json_schema'` (default): Uses Gemini's native structured output API.\n",
            " |\n",
            " |          The Google GenAI SDK automatically transforms schemas to ensure\n",
            " |          compatibility with Gemini. This includes:\n",
            " |\n",
            " |          - Inlining `$defs` definitions (Union types work correctly)\n",
            " |          - Resolving `$ref` references for nested schemas\n",
            " |          - Property ordering preservation\n",
            " |          - Support for streaming partial JSON chunks\n",
            " |\n",
            " |          Uses Gemini's `response_json_schema` API param. Refer to the Gemini API\n",
            " |          [docs](https://ai.google.dev/gemini-api/docs/structured-output) for more\n",
            " |          details. This method is recommended for better reliability as it\n",
            " |          constrains the model's generation process directly.\n",
            " |\n",
            " |      * `method='function_calling'`: Uses tool calling to extract structured data.\n",
            " |          Less reliable than `json_schema` but compatible with all models.\n",
            " |\n",
            " |  ???+ example \"Image input\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#image-input)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      import base64\n",
            " |      import httpx\n",
            " |      from langchain.messages import HumanMessage\n",
            " |\n",
            " |      image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
            " |      image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
            " |      message = HumanMessage(\n",
            " |          content=[\n",
            " |              {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
            " |              {\n",
            " |                  \"type\": \"image_url\",\n",
            " |                  \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
            " |              },\n",
            " |          ]\n",
            " |      )\n",
            " |      ai_msg = model.invoke([message])\n",
            " |      ai_msg.content\n",
            " |      ```\n",
            " |\n",
            " |      ```txt\n",
            " |      The weather in this image appears to be sunny and pleasant. The sky is a bright\n",
            " |      blue with scattered white clouds, suggesting fair weather. The lush green grass\n",
            " |      and trees indicate a warm and possibly slightly breezy day. There are no...\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"PDF input\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#pdf-input)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      import base64\n",
            " |      from langchain.messages import HumanMessage\n",
            " |\n",
            " |      pdf_bytes = open(\"/path/to/your/test.pdf\", \"rb\").read()\n",
            " |      pdf_base64 = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
            " |\n",
            " |      message = HumanMessage(\n",
            " |          content=[\n",
            " |              {\"type\": \"text\", \"text\": \"describe the document in a sentence\"},\n",
            " |              {\n",
            " |                  \"type\": \"file\",\n",
            " |                  \"source_type\": \"base64\",\n",
            " |                  \"mime_type\": \"application/pdf\",\n",
            " |                  \"data\": pdf_base64,\n",
            " |              },\n",
            " |          ]\n",
            " |      )\n",
            " |      ai_msg = model.invoke([message])\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Audio input\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#audio-input)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      import base64\n",
            " |      from langchain.messages import HumanMessage\n",
            " |\n",
            " |      audio_bytes = open(\"/path/to/your/audio.mp3\", \"rb\").read()\n",
            " |      audio_base64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
            " |\n",
            " |      message = HumanMessage(\n",
            " |          content=[\n",
            " |              {\"type\": \"text\", \"text\": \"summarize this audio in a sentence\"},\n",
            " |              {\n",
            " |                  \"type\": \"file\",\n",
            " |                  \"source_type\": \"base64\",\n",
            " |                  \"mime_type\": \"audio/mp3\",\n",
            " |                  \"data\": audio_base64,\n",
            " |              },\n",
            " |          ]\n",
            " |      )\n",
            " |      ai_msg = model.invoke([message])\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Video input\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#video-input)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      import base64\n",
            " |      from langchain.messages import HumanMessage\n",
            " |\n",
            " |      video_bytes = open(\"/path/to/your/video.mp4\", \"rb\").read()\n",
            " |      video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\")\n",
            " |\n",
            " |      message = HumanMessage(\n",
            " |          content=[\n",
            " |              {\n",
            " |                  \"type\": \"text\",\n",
            " |                  \"text\": \"describe what's in this video in a sentence\",\n",
            " |              },\n",
            " |              {\n",
            " |                  \"type\": \"file\",\n",
            " |                  \"source_type\": \"base64\",\n",
            " |                  \"mime_type\": \"video/mp4\",\n",
            " |                  \"data\": video_base64,\n",
            " |              },\n",
            " |          ]\n",
            " |      )\n",
            " |      ai_msg = model.invoke([message])\n",
            " |      ```\n",
            " |\n",
            " |      You can also pass YouTube URLs directly:\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |      from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |\n",
            " |      message = HumanMessage(\n",
            " |          content=[\n",
            " |              {\"type\": \"text\", \"text\": \"Summarize the video in 3 sentences.\"},\n",
            " |              {\n",
            " |                  \"type\": \"media\",\n",
            " |                  \"file_uri\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n",
            " |                  \"mime_type\": \"video/mp4\",\n",
            " |              },\n",
            " |          ]\n",
            " |      )\n",
            " |      response = model.invoke([message])\n",
            " |      print(response.text)\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Image generation\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#image-generation)\n",
            " |      for more info.\n",
            " |\n",
            " |  ???+ example \"Audio generation\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#audio-generation)\n",
            " |      for more info.\n",
            " |\n",
            " |      !!! note \"Vertex compatibility\"\n",
            " |\n",
            " |          Audio generation models (TTS) are currently in preview on Vertex AI\n",
            " |          and may require allowlist access. If you receive an `INVALID_ARGUMENT`\n",
            " |          error when using TTS models with `vertexai=True`, your project may need to\n",
            " |          be allowlisted.\n",
            " |\n",
            " |          See this post on the [Google AI forum](https://discuss.ai.google.dev/t/request-allowlist-access-for-audio-output-in-gemini-2-5-pro-flash-tts-vertex-ai/108067)\n",
            " |          for more details.\n",
            " |\n",
            " |  ???+ example \"File upload\"\n",
            " |\n",
            " |      You can also upload files to Google's servers and reference them by URI.\n",
            " |\n",
            " |      This works for PDFs, images, videos, and audio files.\n",
            " |\n",
            " |      ```python\n",
            " |      import time\n",
            " |      from google import genai\n",
            " |      from langchain.messages import HumanMessage\n",
            " |\n",
            " |      client = genai.Client()\n",
            " |\n",
            " |      myfile = client.files.upload(file=\"/path/to/your/sample.pdf\")\n",
            " |      while myfile.state.name == \"PROCESSING\":\n",
            " |          time.sleep(2)\n",
            " |          myfile = client.files.get(name=myfile.name)\n",
            " |\n",
            " |      message = HumanMessage(\n",
            " |          content=[\n",
            " |              {\"type\": \"text\", \"text\": \"What is in the document?\"},\n",
            " |              {\n",
            " |                  \"type\": \"media\",\n",
            " |                  \"file_uri\": myfile.uri,\n",
            " |                  \"mime_type\": \"application/pdf\",\n",
            " |              },\n",
            " |          ]\n",
            " |      )\n",
            " |      ai_msg = model.invoke([message])\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Thinking\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#thinking-support)\n",
            " |      for more info.\n",
            " |\n",
            " |      Gemini 3+ models use [`thinking_level`][langchain_google_genai.ChatGoogleGenerativeAI.thinking_level]\n",
            " |      (`'low'`, `'medium'`, or `'high'`) to control reasoning depth. If not specified,\n",
            " |      defaults to `'high'`.\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(\n",
            " |          model=\"gemini-3-pro-preview\",\n",
            " |          thinking_level=\"low\",  # For faster, lower-latency responses\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      Gemini 2.5 models use [`thinking_budget`][langchain_google_genai.ChatGoogleGenerativeAI.thinking_budget]\n",
            " |      (an integer token count) to control reasoning. Set to `0` to disable thinking\n",
            " |      (where supported), or `-1` for dynamic thinking.\n",
            " |\n",
            " |      See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for\n",
            " |      more details on thinking models.\n",
            " |\n",
            " |      To see a thinking model's thoughts, set [`include_thoughts=True`][langchain_google_genai.ChatGoogleGenerativeAI.include_thoughts]\n",
            " |      to have the model's reasoning summaries included in the response.\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(\n",
            " |          model=\"gemini-3-pro-preview\",\n",
            " |          include_thoughts=True,\n",
            " |      )\n",
            " |      ai_msg = model.invoke(\"How many 'r's are in the word 'strawberry'?\")\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Thought signatures\"\n",
            " |\n",
            " |      Gemini 3+ models return *thought signatures*—encrypted representations of\n",
            " |      the model's internal reasoning.\n",
            " |\n",
            " |      For multi-turn conversations involving tool calls, you must pass the full\n",
            " |      [`AIMessage`][langchain.messages.AIMessage] back to the model so that these\n",
            " |      signatures are preserved. This happens automatically when you append the\n",
            " |      [`AIMessage`][langchain.messages.AIMessage] to your message list.\n",
            " |\n",
            " |      See the [LangChain docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#thought-signatures) for more info as well as a code example.\n",
            " |\n",
            " |      See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking)\n",
            " |      for more details on thought signatures.\n",
            " |\n",
            " |  ???+ example \"Google search\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#google-search)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |      response = model.invoke(\n",
            " |          \"When is the next total solar eclipse in US?\",\n",
            " |          tools=[{\"google_search\": {}}],\n",
            " |      )\n",
            " |      response.content_blocks\n",
            " |      ```\n",
            " |\n",
            " |      Alternatively, you can bind the tool to the model for easier reuse across calls:\n",
            " |\n",
            " |      ```python\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |\n",
            " |      model_with_search = model.bind_tools([{\"google_search\": {}}])\n",
            " |      response = model_with_search.invoke(\n",
            " |          \"When is the next total solar eclipse in US?\"\n",
            " |      )\n",
            " |\n",
            " |      response.content_blocks\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Google Maps\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#google-maps)\n",
            " |      for more info.\n",
            " |\n",
            " |  ???+ example \"Code execution\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#code-execution)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |      model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |\n",
            " |      model_with_code_interpreter = model.bind_tools([{\"code_execution\": {}}])\n",
            " |      response = model_with_code_interpreter.invoke(\"Use Python to calculate 3^3.\")\n",
            " |\n",
            " |      response.content_blocks\n",
            " |      ```\n",
            " |\n",
            " |      ```output\n",
            " |      [{'type': 'server_tool_call',\n",
            " |        'name': 'code_interpreter',\n",
            " |        'args': {'code': 'print(3**3)', 'language': <Language.PYTHON: 1>},\n",
            " |        'id': '...'},\n",
            " |       {'type': 'server_tool_result',\n",
            " |        'tool_call_id': '',\n",
            " |        'status': 'success',\n",
            " |        'output': '27\\n',\n",
            " |        'extras': {'block_type': 'code_execution_result',\n",
            " |         'outcome': 1}},\n",
            " |       {'type': 'text', 'text': 'The calculation of 3 to the power of 3 is 27.'}]\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Computer use\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#computer-use)\n",
            " |      for more info.\n",
            " |\n",
            " |      !!! warning \"Preview model limitations\"\n",
            " |\n",
            " |          The Computer Use model is in preview and may produce unexpected behavior.\n",
            " |\n",
            " |          Always supervise automated tasks and avoid use with sensitive data or\n",
            " |          critical operations. See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/computer-use)\n",
            " |          for safety best practices.\n",
            " |\n",
            " |  ???+ example \"Token usage\"\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#token-usage-tracking)\n",
            " |      for more info.\n",
            " |\n",
            " |      ```python\n",
            " |      ai_msg = model.invoke(messages)\n",
            " |      ai_msg.usage_metadata\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      {\"input_tokens\": 18, \"output_tokens\": 5, \"total_tokens\": 23}\n",
            " |      ```\n",
            " |\n",
            " |  ???+ example \"Safety settings\"\n",
            " |\n",
            " |      Gemini models have default safety settings that can be overridden. If you\n",
            " |      are receiving lots of \"Safety Warnings\" from your models, you can try\n",
            " |      tweaking the `safety_settings` attribute of the model. For example, to\n",
            " |      turn off safety blocking for dangerous content, you can construct your\n",
            " |      LLM as follows:\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_google_genai import (\n",
            " |          ChatGoogleGenerativeAI,\n",
            " |          HarmBlockThreshold,\n",
            " |          HarmCategory,\n",
            " |      )\n",
            " |\n",
            " |      llm = ChatGoogleGenerativeAI(\n",
            " |          model=\"gemini-3-pro-preview\",\n",
            " |          safety_settings={\n",
            " |              HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
            " |          },\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |      For an enumeration of the categories and thresholds available, see Google's\n",
            " |      [safety setting types](https://ai.google.dev/api/python/google/generativeai/types/SafetySettingDict).\n",
            " |\n",
            " |  ???+ example \"Context caching\"\n",
            " |\n",
            " |      See [the docs](https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai#context-caching)\n",
            " |      for more info.\n",
            " |\n",
            " |      Context caching allows you to store and reuse content (e.g., PDFs, images) for\n",
            " |      faster processing. The [`cached_content`][langchain_google_genai.ChatGoogleGenerativeAI.cached_content]\n",
            " |      parameter accepts a cache name created via the Google Generative AI API.\n",
            " |\n",
            " |      See the Gemini docs for more details on [cached content](https://ai.google.dev/gemini-api/docs/caching?lang=python).\n",
            " |\n",
            " |      Below are two examples: caching a single file directly and caching multiple\n",
            " |      files using `Part`.\n",
            " |\n",
            " |      ???+ example \"Single file example\"\n",
            " |\n",
            " |          This caches a single file and queries it.\n",
            " |\n",
            " |          ```python\n",
            " |          from google import genai\n",
            " |          from google.genai import types\n",
            " |          import time\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |          from langchain.messages import HumanMessage\n",
            " |\n",
            " |          client = genai.Client()\n",
            " |\n",
            " |          # Upload file\n",
            " |          file = client.files.upload(file=\"path/to/your/file\")\n",
            " |          while file.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              file = client.files.get(name=file.name)\n",
            " |\n",
            " |          # Create cache\n",
            " |          model = \"gemini-3-pro-preview\"\n",
            " |          cache = client.caches.create(\n",
            " |              model=model,\n",
            " |              config=types.CreateCachedContentConfig(\n",
            " |                  display_name=\"Cached Content\",\n",
            " |                  system_instruction=(\n",
            " |                      \"You are an expert content analyzer, and your job is to answer \"\n",
            " |                      \"the user's query based on the file you have access to.\"\n",
            " |                  ),\n",
            " |                  contents=[file],\n",
            " |                  ttl=\"300s\",\n",
            " |              ),\n",
            " |          )\n",
            " |\n",
            " |          # Query with LangChain\n",
            " |          llm = ChatGoogleGenerativeAI(\n",
            " |              model=model,\n",
            " |              cached_content=cache.name,\n",
            " |          )\n",
            " |          message = HumanMessage(content=\"Summarize the main points of the content.\")\n",
            " |          llm.invoke([message])\n",
            " |          ```\n",
            " |\n",
            " |      ??? example \"Multiple files example\"\n",
            " |\n",
            " |          This caches two files using `Part` and queries them together.\n",
            " |\n",
            " |          ```python\n",
            " |          from google import genai\n",
            " |          from google.genai.types import CreateCachedContentConfig, Content, Part\n",
            " |          import time\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |          from langchain.messages import HumanMessage\n",
            " |\n",
            " |          client = genai.Client()\n",
            " |\n",
            " |          # Upload files\n",
            " |          file_1 = client.files.upload(file=\"./file1\")\n",
            " |          while file_1.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              file_1 = client.files.get(name=file_1.name)\n",
            " |\n",
            " |          file_2 = client.files.upload(file=\"./file2\")\n",
            " |          while file_2.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              file_2 = client.files.get(name=file_2.name)\n",
            " |\n",
            " |          # Create cache with multiple files\n",
            " |          contents = [\n",
            " |              Content(\n",
            " |                  role=\"user\",\n",
            " |                  parts=[\n",
            " |                      Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),\n",
            " |                      Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),\n",
            " |                  ],\n",
            " |              )\n",
            " |          ]\n",
            " |          model = \"gemini-3-pro-preview\"\n",
            " |          cache = client.caches.create(\n",
            " |              model=model,\n",
            " |              config=CreateCachedContentConfig(\n",
            " |                  display_name=\"Cached Contents\",\n",
            " |                  system_instruction=(\n",
            " |                      \"You are an expert content analyzer, and your job is to answer \"\n",
            " |                      \"the user's query based on the files you have access to.\"\n",
            " |                  ),\n",
            " |                  contents=contents,\n",
            " |                  ttl=\"300s\",\n",
            " |              ),\n",
            " |          )\n",
            " |\n",
            " |          # Query with LangChain\n",
            " |          llm = ChatGoogleGenerativeAI(\n",
            " |              model=model,\n",
            " |              cached_content=cache.name,\n",
            " |          )\n",
            " |          message = HumanMessage(\n",
            " |              content=\"Provide a summary of the key information across both files.\"\n",
            " |          )\n",
            " |          llm.invoke([message])\n",
            " |          ```\n",
            " |\n",
            " |  ???+ example \"Response metadata\"\n",
            " |\n",
            " |      ```python\n",
            " |      ai_msg = model.invoke(messages)\n",
            " |      ai_msg.response_metadata\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      {\n",
            " |          \"model_name\": \"gemini-3-pro-preview\",\n",
            " |          \"model_provider\": \"google_genai\",\n",
            " |          \"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []},\n",
            " |          \"finish_reason\": \"STOP\",\n",
            " |          \"safety_ratings\": [\n",
            " |              {\n",
            " |                  \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                  \"probability\": \"NEGLIGIBLE\",\n",
            " |                  \"blocked\": False,\n",
            " |              },\n",
            " |              {\n",
            " |                  \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                  \"probability\": \"NEGLIGIBLE\",\n",
            " |                  \"blocked\": False,\n",
            " |              },\n",
            " |              {\n",
            " |                  \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                  \"probability\": \"NEGLIGIBLE\",\n",
            " |                  \"blocked\": False,\n",
            " |              },\n",
            " |              {\n",
            " |                  \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                  \"probability\": \"NEGLIGIBLE\",\n",
            " |                  \"blocked\": False,\n",
            " |              },\n",
            " |          ],\n",
            " |      }\n",
            " |      ```\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      ChatGoogleGenerativeAI\n",
            " |      langchain_google_genai._common._BaseGoogleGenerativeAI\n",
            " |      langchain_core.language_models.chat_models.BaseChatModel\n",
            " |      langchain_core.language_models.base.BaseLanguageModel[AIMessage]\n",
            " |      langchain_core.language_models.base.BaseLanguageModel\n",
            " |      langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], TypeVar]\n",
            " |      langchain_core.runnables.base.RunnableSerializable\n",
            " |      langchain_core.load.serializable.Serializable\n",
            " |      pydantic.main.BaseModel\n",
            " |      langchain_core.runnables.base.Runnable\n",
            " |      abc.ABC\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __del__(self) -> 'None'\n",
            " |      Clean up the client on deletion.\n",
            " |\n",
            " |  __init__(self, **kwargs: 'Any') -> 'None'\n",
            " |      Needed for arg validation.\n",
            " |\n",
            " |  bind_tools(self, tools: 'Sequence[dict[str, Any] | type | Callable[..., Any] | BaseTool | GoogleTool]', tool_config: 'dict | ToolConfig | None' = None, *, tool_choice: '_ToolChoiceType | bool | None' = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, AIMessage]'\n",
            " |      Bind tool-like objects to this chat model.\n",
            " |\n",
            " |      Args:\n",
            " |          tools: A list of tool definitions to bind to this chat model.\n",
            " |\n",
            " |              Can be a pydantic model, `Callable`, or `BaseTool`. Pydantic models,\n",
            " |              `Callable`, and `BaseTool` objects will be automatically converted to\n",
            " |              their schema dictionary representation.\n",
            " |\n",
            " |              Tools with Union types in their arguments are now supported and\n",
            " |              converted to `anyOf` schemas.\n",
            " |          tool_config: Optional tool configuration for additional settings like\n",
            " |              `retrieval_config` (for Google Maps/Google Search grounding).\n",
            " |\n",
            " |              Can be used together with `tool_choice`, but cannot specify\n",
            " |              `function_calling_config` in `tool_config` if `tool_choice` is also\n",
            " |              provided (they would conflict).\n",
            " |\n",
            " |              !!! example \"Example with Google Maps grounding\"\n",
            " |\n",
            " |                  ```python\n",
            " |                  from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |                  model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
            " |\n",
            " |                  response = model.invoke(\n",
            " |                      \"What Italian restaurants are near here?\",\n",
            " |                      tools=[{\"google_maps\": {}}],\n",
            " |                      tool_choice=\"required\",\n",
            " |                      tool_config={\n",
            " |                          \"retrieval_config\": {\n",
            " |                              \"lat_lng\": {\n",
            " |                                  \"latitude\": 48.858844,\n",
            " |                                  \"longitude\": 2.294351,\n",
            " |                              }\n",
            " |                          }\n",
            " |                      },\n",
            " |                  )\n",
            " |                  ```\n",
            " |          tool_choice: Control how the model uses tools.\n",
            " |\n",
            " |              Options:\n",
            " |\n",
            " |              - `'auto'` (default): Model decides whether to call functions\n",
            " |              - `'any'` or `'required'`: Model must call a function (both are\n",
            " |                  equivalent)\n",
            " |              - `'none'`: Model cannot call functions\n",
            " |              - `'function_name'`: Model must call the specified function\n",
            " |              - `['fn1', 'fn2']`: Model must call one of the specified functions\n",
            " |              - `True`: Same as `'any'`\n",
            " |\n",
            " |              Can be used together with `tool_config` to control function calling\n",
            " |              while also providing additional configuration like `retrieval_config`.\n",
            " |          **kwargs: Any additional parameters to pass to the `Runnable` constructor.\n",
            " |\n",
            " |  get_num_tokens(self, text: 'str') -> 'int'\n",
            " |      Get the number of tokens present in the text. Uses the model's tokenizer.\n",
            " |\n",
            " |      Useful for checking if an input will fit in a model's context window.\n",
            " |\n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |\n",
            " |      Returns:\n",
            " |          The integer number of tokens in the text.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |          num_tokens = llm.get_num_tokens(\"Hello, world!\")\n",
            " |          print(num_tokens)\n",
            " |          # -> 4\n",
            " |          ```\n",
            " |\n",
            " |  invoke(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, code_execution: 'bool | None' = None, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'AIMessage'\n",
            " |      Override `invoke` on `ChatGoogleGenerativeAI` to add `code_execution`.\n",
            " |\n",
            " |  validate_environment(self) -> 'Self'\n",
            " |      Validates params and builds client.\n",
            " |\n",
            " |      We override `temperature` to `1.0` for Gemini 3+ models if not explicitly set.\n",
            " |      This is to prevent infinite loops and degraded performance that can occur with\n",
            " |      `temperature < 1.0` on these models.\n",
            " |\n",
            " |  with_structured_output(self, schema: 'dict | type[BaseModel]', method: \"Literal['function_calling', 'json_mode', 'json_schema'] | None\" = 'json_schema', *, include_raw: 'bool' = False, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, dict | BaseModel]'\n",
            " |      Return a `Runnable` that constrains model output to a given schema.\n",
            " |\n",
            " |      Constrains the model to return output conforming to the provided schema.\n",
            " |\n",
            " |      Supports Pydantic models, `TypedDict`, and JSON schema dictionaries.\n",
            " |\n",
            " |      Args:\n",
            " |          schema: The output schema as a Pydantic `BaseModel` class, a `TypedDict`\n",
            " |              class, or a JSON schema dictionary.\n",
            " |          method: The method to use for structured output.\n",
            " |\n",
            " |              Options:\n",
            " |\n",
            " |              - `'json_schema'` (recommended): Uses native JSON schema support for\n",
            " |                  reliable structured output. Supports streaming with fully-parsed\n",
            " |                  Pydantic objects.\n",
            " |              - `'json_mode'`: Deprecated alias for `'json_schema'`.\n",
            " |              - `'function_calling'`: Uses tool/function calling. Less reliable than\n",
            " |                  `'json_schema'` and not recommended for new code.\n",
            " |          include_raw: If `True`, returns a dict with both the raw model output\n",
            " |              and the parsed structured output.\n",
            " |\n",
            " |      Returns:\n",
            " |          A `Runnable` that takes the same input as the chat model but returns the\n",
            " |              structured output. When streaming, emits fully-parsed objects of the\n",
            " |              specified schema type (not incremental JSON strings).\n",
            " |\n",
            " |      Example:\n",
            " |          ```python title=\"Basic usage with Pydantic model\"\n",
            " |          from pydantic import BaseModel\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |\n",
            " |          class Person(BaseModel):\n",
            " |              name: str\n",
            " |              age: int\n",
            " |\n",
            " |\n",
            " |          model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |          structured_model = model.with_structured_output(\n",
            " |              Person,\n",
            " |              method=\"json_schema\",\n",
            " |          )\n",
            " |\n",
            " |          result = structured_model.invoke(\n",
            " |              \"Tell me about a person named Alice, age 30\"\n",
            " |          )\n",
            " |          print(result)  # Person(name=\"Alice\", age=30)\n",
            " |          ```\n",
            " |\n",
            " |          ```python title=\"Streaming structured output\"\n",
            " |          from pydantic import BaseModel\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |\n",
            " |          class Recipe(BaseModel):\n",
            " |              name: str\n",
            " |              ingredients: list[str]\n",
            " |              steps: list[str]\n",
            " |\n",
            " |\n",
            " |          model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |          structured_model = model.with_structured_output(\n",
            " |              Recipe, method=\"json_schema\"\n",
            " |          )\n",
            " |\n",
            " |          # Emits fully-parsed Recipe objects, not incremental JSON strings\n",
            " |          for chunk in structured_model.stream(\n",
            " |              \"Give me a recipe for chocolate chip cookies\"\n",
            " |          ):\n",
            " |              print(chunk)  # Recipe(name=..., ingredients=[...], steps=[...])\n",
            " |          ```\n",
            " |\n",
            " |          ```python title=\"Using with dict schema\"\n",
            " |          model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")\n",
            " |\n",
            " |          schema = {\n",
            " |              \"type\": \"object\",\n",
            " |              \"properties\": {\n",
            " |                  \"title\": {\"type\": \"string\"},\n",
            " |                  \"priority\": {\"type\": \"integer\"},\n",
            " |              },\n",
            " |              \"required\": [\"title\", \"priority\"],\n",
            " |          }\n",
            " |\n",
            " |          structured_model = model.with_structured_output(\n",
            " |              schema, method=\"json_schema\"\n",
            " |          )\n",
            " |          result = structured_model.invoke(\"Create a task: finish report, priority 1\")\n",
            " |          print(result)  # {\"title\": \"finish report\", \"priority\": 1}\n",
            " |          ```\n",
            " |\n",
            " |          ```python title=\"Including raw output\"\n",
            " |          structured_model = model.with_structured_output(\n",
            " |              Person, method=\"json_schema\", include_raw=True\n",
            " |          )\n",
            " |\n",
            " |          result = structured_model.invoke(\"Tell me about Bob, age 25\")\n",
            " |          print(result[\"parsed\"])  # Person(name=\"Bob\", age=25)\n",
            " |          print(result[\"raw\"])  # AIMessage with full model response\n",
            " |          ```\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |\n",
            " |  build_extra(values: 'dict[str, Any]') -> 'Any'\n",
            " |      Build extra kwargs from additional params that were passed in.\n",
            " |\n",
            " |      (In other words, handle additional params that aren't explicitly defined as\n",
            " |      model fields. Used to pass extra config to underlying APIs without defining them\n",
            " |      all here.)\n",
            " |\n",
            " |  is_lc_serializable() -> 'bool'\n",
            " |      Is this class serializable?\n",
            " |\n",
            " |      By design, even if a class inherits from `Serializable`, it is not serializable\n",
            " |      by default. This is to prevent accidental serialization of objects that should\n",
            " |      not be serialized.\n",
            " |\n",
            " |      Returns:\n",
            " |          Whether the class is serializable. Default is `False`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |\n",
            " |  async_client\n",
            " |      Async client for Google GenAI operations..\n",
            " |\n",
            " |      Returns:\n",
            " |          The async client interface that exposes async versions of all client\n",
            " |              methods.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValueError: If the client has not been initialized.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  __annotations__ = {'cached_content': 'str | None', 'client': 'Client |...\n",
            " |\n",
            " |  __class_vars__ = set()\n",
            " |\n",
            " |  __parameters__ = ()\n",
            " |\n",
            " |  __private_attributes__ = {}\n",
            " |\n",
            " |  __pydantic_complete__ = True\n",
            " |\n",
            " |  __pydantic_computed_fields__ = {}\n",
            " |\n",
            " |  __pydantic_core_schema__ = {'function': {'function': <function ChatGoo...\n",
            " |\n",
            " |  __pydantic_custom_init__ = True\n",
            " |\n",
            " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
            " |\n",
            " |  __pydantic_fields__ = {'additional_headers': FieldInfo(annotation=Unio...\n",
            " |\n",
            " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
            " |\n",
            " |  __pydantic_parent_namespace__ = None\n",
            " |\n",
            " |  __pydantic_post_init__ = None\n",
            " |\n",
            " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
            " |      Model...\n",
            " |\n",
            " |  __pydantic_setattr_handlers__ = {}\n",
            " |\n",
            " |  __pydantic_validator__ = SchemaValidator(title=\"ChatGoogleGenerativeAI...\n",
            " |\n",
            " |  __signature__ = <Signature (*, name: str | None = None, cache: l...Non...\n",
            " |\n",
            " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_google_genai._common._BaseGoogleGenerativeAI:\n",
            " |\n",
            " |  lc_secrets\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from langchain_google_genai._common._BaseGoogleGenerativeAI:\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |\n",
            " |  async agenerate(self, messages: 'list[list[BaseMessage]]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, *, tags: 'list[str] | None' = None, metadata: 'dict[str, Any] | None' = None, run_name: 'str | None' = None, run_id: 'uuid.UUID | None' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |          type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating.\n",
            " |\n",
            " |              Model output is cut off at the first occurrence of any of these\n",
            " |              substrings.\n",
            " |          callbacks: `Callbacks` to pass through.\n",
            " |\n",
            " |              Used for executing additional functionality, such as logging or\n",
            " |              streaming, throughout generation.\n",
            " |          tags: The tags to apply.\n",
            " |          metadata: The metadata to apply.\n",
            " |          run_name: The name of the run.\n",
            " |          run_id: The ID of the run.\n",
            " |          **kwargs: Arbitrary additional keyword arguments.\n",
            " |\n",
            " |              These are usually passed to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An `LLMResult`, which contains a list of candidate `Generations` for each\n",
            " |              input prompt and additional model provider-specific output.\n",
            " |\n",
            " |  async agenerate_prompt(self, prompts: 'list[PromptValue]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts and return model generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |          type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          prompts: List of `PromptValue` objects.\n",
            " |\n",
            " |              A `PromptValue` is an object that can be converted to match the format\n",
            " |              of any language model (string for pure text generation models and\n",
            " |              `BaseMessage` objects for chat models).\n",
            " |          stop: Stop words to use when generating.\n",
            " |\n",
            " |              Model output is cut off at the first occurrence of any of these\n",
            " |              substrings.\n",
            " |          callbacks: `Callbacks` to pass through.\n",
            " |\n",
            " |              Used for executing additional functionality, such as logging or\n",
            " |              streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments.\n",
            " |\n",
            " |              These are usually passed to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An `LLMResult`, which contains a list of candidate `Generation` objects for\n",
            " |              each input prompt and additional model provider-specific output.\n",
            " |\n",
            " |  async ainvoke(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'AIMessage'\n",
            " |      Transform a single input into an output.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the `Runnable`.\n",
            " |          config: A config to use when invoking the `Runnable`.\n",
            " |\n",
            " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
            " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
            " |              do in parallel, and other keys.\n",
            " |\n",
            " |              Please refer to `RunnableConfig` for more details.\n",
            " |\n",
            " |      Returns:\n",
            " |          The output of the `Runnable`.\n",
            " |\n",
            " |  async astream(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[AIMessageChunk]'\n",
            " |      Default implementation of `astream`, which calls `ainvoke`.\n",
            " |\n",
            " |      Subclasses must override this method if they support streaming output.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the `Runnable`.\n",
            " |          config: The config to use for the `Runnable`.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the `Runnable`.\n",
            " |\n",
            " |  dict(self, **kwargs: 'Any') -> 'dict'\n",
            " |      Return a dictionary of the LLM.\n",
            " |\n",
            " |  generate(self, messages: 'list[list[BaseMessage]]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, *, tags: 'list[str] | None' = None, metadata: 'dict[str, Any] | None' = None, run_name: 'str | None' = None, run_id: 'uuid.UUID | None' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |          type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating.\n",
            " |\n",
            " |              Model output is cut off at the first occurrence of any of these\n",
            " |              substrings.\n",
            " |          callbacks: `Callbacks` to pass through.\n",
            " |\n",
            " |              Used for executing additional functionality, such as logging or\n",
            " |              streaming, throughout generation.\n",
            " |          tags: The tags to apply.\n",
            " |          metadata: The metadata to apply.\n",
            " |          run_name: The name of the run.\n",
            " |          run_id: The ID of the run.\n",
            " |          **kwargs: Arbitrary additional keyword arguments.\n",
            " |\n",
            " |              These are usually passed to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An `LLMResult`, which contains a list of candidate `Generations` for each\n",
            " |              input prompt and additional model provider-specific output.\n",
            " |\n",
            " |  generate_prompt(self, prompts: 'list[PromptValue]', stop: 'list[str] | None' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |          type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          prompts: List of `PromptValue` objects.\n",
            " |\n",
            " |              A `PromptValue` is an object that can be converted to match the format\n",
            " |              of any language model (string for pure text generation models and\n",
            " |              `BaseMessage` objects for chat models).\n",
            " |          stop: Stop words to use when generating.\n",
            " |\n",
            " |              Model output is cut off at the first occurrence of any of these\n",
            " |              substrings.\n",
            " |          callbacks: `Callbacks` to pass through.\n",
            " |\n",
            " |              Used for executing additional functionality, such as logging or\n",
            " |              streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments.\n",
            " |\n",
            " |              These are usually passed to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An `LLMResult`, which contains a list of candidate `Generation` objects for\n",
            " |              each input prompt and additional model provider-specific output.\n",
            " |\n",
            " |  stream(self, input: 'LanguageModelInput', config: 'RunnableConfig | None' = None, *, stop: 'list[str] | None' = None, **kwargs: 'Any') -> 'Iterator[AIMessageChunk]'\n",
            " |      Default implementation of `stream`, which calls `invoke`.\n",
            " |\n",
            " |      Subclasses must override this method if they support streaming output.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the `Runnable`.\n",
            " |          config: The config to use for the `Runnable`.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the `Runnable`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |\n",
            " |  OutputType\n",
            " |      Get the output type for this `Runnable`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |\n",
            " |  get_num_tokens_from_messages(self, messages: 'list[BaseMessage]', tools: 'Sequence | None' = None) -> 'int'\n",
            " |      Get the number of tokens in the messages.\n",
            " |\n",
            " |      Useful for checking if an input fits in a model's context window.\n",
            " |\n",
            " |      This should be overridden by model-specific implementations to provide accurate\n",
            " |      token counts via model-specific tokenizers.\n",
            " |\n",
            " |      !!! note\n",
            " |\n",
            " |          * The base implementation of `get_num_tokens_from_messages` ignores tool\n",
            " |              schemas.\n",
            " |          * The base implementation of `get_num_tokens_from_messages` adds additional\n",
            " |              prefixes to messages in represent user roles, which will add to the\n",
            " |              overall token count. Model-specific implementations may choose to\n",
            " |              handle this differently.\n",
            " |\n",
            " |      Args:\n",
            " |          messages: The message inputs to tokenize.\n",
            " |          tools: If provided, sequence of dict, `BaseModel`, function, or\n",
            " |              `BaseTool` objects to be converted to tool schemas.\n",
            " |\n",
            " |      Returns:\n",
            " |          The sum of the number of tokens across the messages.\n",
            " |\n",
            " |  get_token_ids(self, text: 'str') -> 'list[int]'\n",
            " |      Return the ordered IDs of the tokens in a text.\n",
            " |\n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of IDs corresponding to the tokens in the text, in order they occur\n",
            " |              in the text.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |\n",
            " |  set_verbose(verbose: 'bool | None') -> 'bool'\n",
            " |      If verbose is `None`, set it.\n",
            " |\n",
            " |      This allows users to pass in `None` as verbose to access the global setting.\n",
            " |\n",
            " |      Args:\n",
            " |          verbose: The verbosity setting to use.\n",
            " |\n",
            " |      Returns:\n",
            " |          The verbosity setting to use.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |\n",
            " |  InputType\n",
            " |      Get the input type for this `Runnable`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |\n",
            " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure alternatives for `Runnable` objects that can be set at runtime.\n",
            " |\n",
            " |      Args:\n",
            " |          which: The `ConfigurableField` instance that will be used to select the\n",
            " |              alternative.\n",
            " |          default_key: The default key to use if no alternative is selected.\n",
            " |          prefix_keys: Whether to prefix the keys with the `ConfigurableField` id.\n",
            " |          **kwargs: A dictionary of keys to `Runnable` instances or callables that\n",
            " |              return `Runnable` instances.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the alternatives configured.\n",
            " |\n",
            " |      !!! example\n",
            " |\n",
            " |          ```python\n",
            " |          from langchain_anthropic import ChatAnthropic\n",
            " |          from langchain_core.runnables.utils import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |\n",
            " |          model = ChatAnthropic(\n",
            " |              model_name=\"claude-sonnet-4-5-20250929\"\n",
            " |          ).configurable_alternatives(\n",
            " |              ConfigurableField(id=\"llm\"),\n",
            " |              default_key=\"anthropic\",\n",
            " |              openai=ChatOpenAI(),\n",
            " |          )\n",
            " |\n",
            " |          # uses the default model ChatAnthropic\n",
            " |          print(model.invoke(\"which organization created you?\").content)\n",
            " |\n",
            " |          # uses ChatOpenAI\n",
            " |          print(\n",
            " |              model.with_config(configurable={\"llm\": \"openai\"})\n",
            " |              .invoke(\"which organization created you?\")\n",
            " |              .content\n",
            " |          )\n",
            " |          ```\n",
            " |\n",
            " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure particular `Runnable` fields at runtime.\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: A dictionary of `ConfigurableField` instances to configure.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValueError: If a configuration key is not found in the `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the fields configured.\n",
            " |\n",
            " |      !!! example\n",
            " |\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |\n",
            " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
            " |              max_tokens=ConfigurableField(\n",
            " |                  id=\"output_token_number\",\n",
            " |                  name=\"Max tokens in the output\",\n",
            " |                  description=\"The maximum number of tokens in the output\",\n",
            " |              )\n",
            " |          )\n",
            " |\n",
            " |          # max_tokens = 20\n",
            " |          print(\n",
            " |              \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content\n",
            " |          )\n",
            " |\n",
            " |          # max_tokens = 200\n",
            " |          print(\n",
            " |              \"max_tokens_200: \",\n",
            " |              model.with_config(configurable={\"output_token_number\": 200})\n",
            " |              .invoke(\"tell me something about chess\")\n",
            " |              .content,\n",
            " |          )\n",
            " |          ```\n",
            " |\n",
            " |  to_json(self) -> 'SerializedConstructor | SerializedNotImplemented'\n",
            " |      Serialize the `Runnable` to JSON.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON-serializable representation of the `Runnable`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |\n",
            " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |\n",
            " |  __repr_args__(self) -> Any\n",
            " |\n",
            " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
            " |      Serialize a \"not implemented\" object.\n",
            " |\n",
            " |      Returns:\n",
            " |          `SerializedNotImplemented`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |\n",
            " |  get_lc_namespace() -> list[str]\n",
            " |      Get the namespace of the LangChain object.\n",
            " |\n",
            " |      For example, if the class is [`langchain.llms.openai.OpenAI`][langchain_openai.OpenAI],\n",
            " |      then the namespace is `[\"langchain\", \"llms\", \"openai\"]`\n",
            " |\n",
            " |      Returns:\n",
            " |          The namespace.\n",
            " |\n",
            " |  lc_id() -> list[str]\n",
            " |      Return a unique identifier for this class for serialization purposes.\n",
            " |\n",
            " |      The unique identifier is a list of strings that describes the path\n",
            " |      to the object.\n",
            " |\n",
            " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
            " |      `[\"langchain\", \"llms\", \"openai\", \"OpenAI\"]`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
            " |\n",
            " |  lc_attributes\n",
            " |      List of attribute names that should be included in the serialized kwargs.\n",
            " |\n",
            " |      These attributes must be accepted by the constructor.\n",
            " |\n",
            " |      Default is an empty dictionary.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __copy__(self) -> 'Self'\n",
            " |      Returns a shallow copy of the model.\n",
            " |\n",
            " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
            " |      Returns a deep copy of the model.\n",
            " |\n",
            " |  __delattr__(self, item: 'str') -> 'Any'\n",
            " |      Implement delattr(self, name).\n",
            " |\n",
            " |  __eq__(self, other: 'Any') -> 'bool'\n",
            " |      Return self==value.\n",
            " |\n",
            " |  __getattr__(self, item: 'str') -> 'Any'\n",
            " |\n",
            " |  __getstate__(self) -> 'dict[Any, Any]'\n",
            " |      Helper for pickle.\n",
            " |\n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      So `dict(model)` works.\n",
            " |\n",
            " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
            " |\n",
            " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
            " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
            " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
            " |\n",
            " |  __repr__(self) -> 'str'\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |\n",
            " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
            " |      Returns the string representation of a recursive object.\n",
            " |\n",
            " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
            " |\n",
            " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
            " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
            " |\n",
            " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
            " |      Implement setattr(self, name, value).\n",
            " |\n",
            " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
            " |\n",
            " |  __str__(self) -> 'str'\n",
            " |      Return str(self).\n",
            " |\n",
            " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Returns a copy of the model.\n",
            " |\n",
            " |      !!! warning \"Deprecated\"\n",
            " |          This method is now deprecated; use `model_copy` instead.\n",
            " |\n",
            " |      If you need `include` or `exclude`, use:\n",
            " |\n",
            " |      ```python {test=\"skip\" lint=\"skip\"}\n",
            " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
            " |      data = {**data, **(update or {})}\n",
            " |      copied = self.model_validate(data)\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
            " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
            " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
            " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
            " |\n",
            " |      Returns:\n",
            " |          A copy of the model with included, excluded and updated fields as specified.\n",
            " |\n",
            " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
            " |\n",
            " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_copy`](../concepts/models.md#model-copy)\n",
            " |\n",
            " |      Returns a copy of the model.\n",
            " |\n",
            " |      !!! note\n",
            " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
            " |          might have unexpected side effects if you store anything in it, on top of the model\n",
            " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
            " |\n",
            " |      Args:\n",
            " |          update: Values to change/add in the new model. Note: the data is not validated\n",
            " |              before creating the new model. You should trust this data.\n",
            " |          deep: Set to `True` to make a deep copy of the model.\n",
            " |\n",
            " |      Returns:\n",
            " |          New model instance.\n",
            " |\n",
            " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, exclude_computed_fields: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
            " |\n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |\n",
            " |      Args:\n",
            " |          mode: The mode in which `to_python` should run.\n",
            " |              If mode is 'json', the output will only contain JSON serializable types.\n",
            " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
            " |          include: A set of fields to include in the output.\n",
            " |          exclude: A set of fields to exclude from the output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          exclude_computed_fields: Whether to exclude computed fields.\n",
            " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
            " |              `round_trip` parameter instead.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
            " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dictionary representation of the model.\n",
            " |\n",
            " |  model_dump_json(self, *, indent: 'int | None' = None, ensure_ascii: 'bool' = False, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, exclude_computed_fields: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
            " |\n",
            " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
            " |\n",
            " |      Args:\n",
            " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
            " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
            " |              If `False` (the default), these characters will be output as-is.\n",
            " |          include: Field(s) to include in the JSON output.\n",
            " |          exclude: Field(s) to exclude from the JSON output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to serialize using field aliases.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          exclude_computed_fields: Whether to exclude computed fields.\n",
            " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
            " |              `round_trip` parameter instead.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
            " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON string representation of the model.\n",
            " |\n",
            " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
            " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
            " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
            " |      Parameterizes a generic class.\n",
            " |\n",
            " |      At least, parameterizing a generic class is the *main* thing this\n",
            " |      method does. For example, for some generic class `Foo`, this is called\n",
            " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
            " |\n",
            " |      However, note that this method is also called when defining generic\n",
            " |      classes in the first place with `class Foo[T]: ...`.\n",
            " |\n",
            " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
            " |\n",
            " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
            " |      Hook into generating the model's JSON schema.\n",
            " |\n",
            " |      Args:\n",
            " |          core_schema: A `pydantic-core` CoreSchema.\n",
            " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
            " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
            " |              or just call the handler with the original schema.\n",
            " |          handler: Call into Pydantic's internal JSON schema generation.\n",
            " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
            " |              generation fails.\n",
            " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
            " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
            " |              for a type.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema, as a Python object.\n",
            " |\n",
            " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
            " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
            " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
            " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
            " |      meaning that creating an instance of the class may fail.\n",
            " |\n",
            " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
            " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
            " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
            " |\n",
            " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
            " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
            " |              by Pydantic.\n",
            " |\n",
            " |      Note:\n",
            " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
            " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
            " |\n",
            " |  __pydantic_on_complete__() -> 'None'\n",
            " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
            " |\n",
            " |      This typically happens when the class is created (just before\n",
            " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
            " |      except when forward annotations are used that could not immediately be resolved.\n",
            " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
            " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
            " |\n",
            " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
            " |\n",
            " |  from_orm(obj: 'Any') -> 'Self'\n",
            " |\n",
            " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
            " |      Creates a new instance of the `Model` class with validated data.\n",
            " |\n",
            " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |\n",
            " |      !!! note\n",
            " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
            " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
            " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
            " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
            " |          an error if extra values are passed, but they will be ignored.\n",
            " |\n",
            " |      Args:\n",
            " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
            " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
            " |              Otherwise, the field names from the `values` argument will be used.\n",
            " |          values: Trusted or pre-validated data dictionary.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new instance of the `Model` class with validated data.\n",
            " |\n",
            " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation', *, union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of') -> 'dict[str, Any]'\n",
            " |      Generates a JSON schema for a model class.\n",
            " |\n",
            " |      Args:\n",
            " |          by_alias: Whether to use attribute aliases or not.\n",
            " |          ref_template: The reference template.\n",
            " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
            " |\n",
            " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
            " |              keyword to combine schemas (the default).\n",
            " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
            " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
            " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
            " |              `any_of`.\n",
            " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
            " |              `GenerateJsonSchema` with your desired modifications\n",
            " |          mode: The mode in which to generate the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          The JSON schema for the given model class.\n",
            " |\n",
            " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
            " |      Compute the class name for parametrizations of generic classes.\n",
            " |\n",
            " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
            " |\n",
            " |      Args:\n",
            " |          params: Tuple of types of the class. Given a generic class\n",
            " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
            " |              the value `(str, int)` would be passed to `params`.\n",
            " |\n",
            " |      Returns:\n",
            " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
            " |\n",
            " |      Raises:\n",
            " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
            " |\n",
            " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
            " |      Try to rebuild the pydantic-core schema for the model.\n",
            " |\n",
            " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
            " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
            " |\n",
            " |      Args:\n",
            " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
            " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
            " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
            " |          _types_namespace: The types namespace, defaults to `None`.\n",
            " |\n",
            " |      Returns:\n",
            " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
            " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
            " |\n",
            " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      Validate a pydantic model instance.\n",
            " |\n",
            " |      Args:\n",
            " |          obj: The object to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
            " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
            " |          from_attributes: Whether to extract data from object attributes.\n",
            " |          context: Additional context to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValidationError: If the object could not be validated.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated model instance.\n",
            " |\n",
            " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
            " |\n",
            " |      Validate the given JSON data against the Pydantic model.\n",
            " |\n",
            " |      Args:\n",
            " |          json_data: The JSON data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
            " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
            " |\n",
            " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      Validate the given object with string data against the Pydantic model.\n",
            " |\n",
            " |      Args:\n",
            " |          obj: The object containing string data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
            " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |\n",
            " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
            " |\n",
            " |  parse_obj(obj: 'Any') -> 'Self'\n",
            " |\n",
            " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
            " |\n",
            " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
            " |\n",
            " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
            " |\n",
            " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
            " |\n",
            " |  validate(value: 'Any') -> 'Self'\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __fields_set__\n",
            " |\n",
            " |  model_extra\n",
            " |      Get extra fields set during validation.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
            " |\n",
            " |  model_fields_set\n",
            " |      Returns the set of fields that have been explicitly set on this model instance.\n",
            " |\n",
            " |      Returns:\n",
            " |          A set of strings representing the fields that have been set,\n",
            " |              i.e. that were not filled from defaults.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __pydantic_extra__\n",
            " |\n",
            " |  __pydantic_fields_set__\n",
            " |\n",
            " |  __pydantic_private__\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __hash__ = None\n",
            " |\n",
            " |  __pydantic_root_model__ = False\n",
            " |\n",
            " |  model_computed_fields = {}\n",
            " |\n",
            " |  model_fields = {'additional_headers': FieldInfo(annotation=Union[dict[...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
            " |\n",
            " |  __or__(self, other: 'Runnable[Any, Other] | Callable[[Iterator[Any]], Iterator[Other]] | Callable[[AsyncIterator[Any]], AsyncIterator[Other]] | Callable[[Any], Other] | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any]') -> 'RunnableSerializable[Input, Other]'\n",
            " |      Runnable \"or\" operator.\n",
            " |\n",
            " |      Compose this `Runnable` with another object to create a\n",
            " |      `RunnableSequence`.\n",
            " |\n",
            " |      Args:\n",
            " |          other: Another `Runnable` or a `Runnable`-like object.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable`.\n",
            " |\n",
            " |  __ror__(self, other: 'Runnable[Other, Any] | Callable[[Iterator[Other]], Iterator[Any]] | Callable[[AsyncIterator[Other]], AsyncIterator[Any]] | Callable[[Other], Any] | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any]') -> 'RunnableSerializable[Other, Output]'\n",
            " |      Runnable \"reverse-or\" operator.\n",
            " |\n",
            " |      Compose this `Runnable` with another object to create a\n",
            " |      `RunnableSequence`.\n",
            " |\n",
            " |      Args:\n",
            " |          other: Another `Runnable` or a `Runnable`-like object.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable`.\n",
            " |\n",
            " |  async abatch(self, inputs: 'list[Input]', config: 'RunnableConfig | list[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'list[Output]'\n",
            " |      Default implementation runs `ainvoke` in parallel using `asyncio.gather`.\n",
            " |\n",
            " |      The default implementation of `batch` works well for IO bound runnables.\n",
            " |\n",
            " |      Subclasses must override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying `Runnable` uses an API which supports a batch mode.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the `Runnable`.\n",
            " |          config: A config to use when invoking the `Runnable`.\n",
            " |\n",
            " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
            " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
            " |              do in parallel, and other keys.\n",
            " |\n",
            " |              Please refer to `RunnableConfig` for more details.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of outputs from the `Runnable`.\n",
            " |\n",
            " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'RunnableConfig | Sequence[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'AsyncIterator[tuple[int, Output | Exception]]'\n",
            " |      Run `ainvoke` in parallel on a list of inputs.\n",
            " |\n",
            " |      Yields results as they complete.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the `Runnable`.\n",
            " |          config: A config to use when invoking the `Runnable`.\n",
            " |\n",
            " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
            " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
            " |              do in parallel, and other keys.\n",
            " |\n",
            " |              Please refer to `RunnableConfig` for more details.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          A tuple of the index of the input and the output from the `Runnable`.\n",
            " |\n",
            " |  as_tool(self, args_schema: 'type[BaseModel] | None' = None, *, name: 'str | None' = None, description: 'str | None' = None, arg_types: 'dict[str, type] | None' = None) -> 'BaseTool'\n",
            " |      .. beta::\n",
            " |         This API is in beta and may change in the future.\n",
            " |\n",
            " |      Create a `BaseTool` from a `Runnable`.\n",
            " |\n",
            " |      `as_tool` will instantiate a `BaseTool` with a name, description, and\n",
            " |      `args_schema` from a `Runnable`. Where possible, schemas are inferred\n",
            " |      from `runnable.get_input_schema`.\n",
            " |\n",
            " |      Alternatively (e.g., if the `Runnable` takes a dict as input and the specific\n",
            " |      `dict` keys are not typed), the schema can be specified directly with\n",
            " |      `args_schema`.\n",
            " |\n",
            " |      You can also pass `arg_types` to just specify the required arguments and their\n",
            " |      types.\n",
            " |\n",
            " |      Args:\n",
            " |          args_schema: The schema for the tool.\n",
            " |          name: The name of the tool.\n",
            " |          description: The description of the tool.\n",
            " |          arg_types: A dictionary of argument names to types.\n",
            " |\n",
            " |      Returns:\n",
            " |          A `BaseTool` instance.\n",
            " |\n",
            " |      !!! example \"`TypedDict` input\"\n",
            " |\n",
            " |          ```python\n",
            " |          from typing_extensions import TypedDict\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          class Args(TypedDict):\n",
            " |              a: int\n",
            " |              b: list[int]\n",
            " |\n",
            " |\n",
            " |          def f(x: Args) -> str:\n",
            " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(f)\n",
            " |          as_tool = runnable.as_tool()\n",
            " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
            " |          ```\n",
            " |\n",
            " |      !!! example \"`dict` input, specifying schema via `args_schema`\"\n",
            " |\n",
            " |          ```python\n",
            " |          from typing import Any\n",
            " |          from pydantic import BaseModel, Field\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |          def f(x: dict[str, Any]) -> str:\n",
            " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
            " |\n",
            " |          class FSchema(BaseModel):\n",
            " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
            " |\n",
            " |              a: int = Field(..., description=\"Integer\")\n",
            " |              b: list[int] = Field(..., description=\"List of ints\")\n",
            " |\n",
            " |          runnable = RunnableLambda(f)\n",
            " |          as_tool = runnable.as_tool(FSchema)\n",
            " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
            " |          ```\n",
            " |\n",
            " |      !!! example \"`dict` input, specifying schema via `arg_types`\"\n",
            " |\n",
            " |          ```python\n",
            " |          from typing import Any\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def f(x: dict[str, Any]) -> str:\n",
            " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(f)\n",
            " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n",
            " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
            " |          ```\n",
            " |\n",
            " |      !!! example \"`str` input\"\n",
            " |\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def f(x: str) -> str:\n",
            " |              return x + \"a\"\n",
            " |\n",
            " |\n",
            " |          def g(x: str) -> str:\n",
            " |              return x + \"z\"\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(f) | g\n",
            " |          as_tool = runnable.as_tool()\n",
            " |          as_tool.invoke(\"b\")\n",
            " |          ```\n",
            " |\n",
            " |  assign(self, **kwargs: 'Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Assigns new fields to the `dict` output of this `Runnable`.\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_core.language_models.fake import FakeStreamingListLLM\n",
            " |      from langchain_core.output_parsers import StrOutputParser\n",
            " |      from langchain_core.prompts import SystemMessagePromptTemplate\n",
            " |      from langchain_core.runnables import Runnable\n",
            " |      from operator import itemgetter\n",
            " |\n",
            " |      prompt = (\n",
            " |          SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
            " |          + \"{question}\"\n",
            " |      )\n",
            " |      model = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
            " |\n",
            " |      chain: Runnable = prompt | model | {\"str\": StrOutputParser()}\n",
            " |\n",
            " |      chain_with_assign = chain.assign(hello=itemgetter(\"str\") | model)\n",
            " |\n",
            " |      print(chain_with_assign.input_schema.model_json_schema())\n",
            " |      # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
            " |      {'question': {'title': 'Question', 'type': 'string'}}}\n",
            " |      print(chain_with_assign.output_schema.model_json_schema())\n",
            " |      # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
            " |      {'str': {'title': 'Str',\n",
            " |      'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: A mapping of keys to `Runnable` or `Runnable`-like objects\n",
            " |              that will be invoked with the entire output dict of this `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable`.\n",
            " |\n",
            " |  async astream_events(self, input: 'Any', config: 'RunnableConfig | None' = None, *, version: \"Literal['v1', 'v2']\" = 'v2', include_names: 'Sequence[str] | None' = None, include_types: 'Sequence[str] | None' = None, include_tags: 'Sequence[str] | None' = None, exclude_names: 'Sequence[str] | None' = None, exclude_types: 'Sequence[str] | None' = None, exclude_tags: 'Sequence[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
            " |      Generate a stream of events.\n",
            " |\n",
            " |      Use to create an iterator over `StreamEvent` that provide real-time information\n",
            " |      about the progress of the `Runnable`, including `StreamEvent` from intermediate\n",
            " |      results.\n",
            " |\n",
            " |      A `StreamEvent` is a dictionary with the following schema:\n",
            " |\n",
            " |      - `event`: Event names are of the format:\n",
            " |          `on_[runnable_type]_(start|stream|end)`.\n",
            " |      - `name`: The name of the `Runnable` that generated the event.\n",
            " |      - `run_id`: Randomly generated ID associated with the given execution of the\n",
            " |          `Runnable` that emitted the event. A child `Runnable` that gets invoked as\n",
            " |          part of the execution of a parent `Runnable` is assigned its own unique ID.\n",
            " |      - `parent_ids`: The IDs of the parent runnables that generated the event. The\n",
            " |          root `Runnable` will have an empty list. The order of the parent IDs is from\n",
            " |          the root to the immediate parent. Only available for v2 version of the API.\n",
            " |          The v1 version of the API will return an empty list.\n",
            " |      - `tags`: The tags of the `Runnable` that generated the event.\n",
            " |      - `metadata`: The metadata of the `Runnable` that generated the event.\n",
            " |      - `data`: The data associated with the event. The contents of this field\n",
            " |          depend on the type of event. See the table below for more details.\n",
            " |\n",
            " |      Below is a table that illustrates some events that might be emitted by various\n",
            " |      chains. Metadata fields have been omitted from the table for brevity.\n",
            " |      Chain definitions have been included after the table.\n",
            " |\n",
            " |      !!! note\n",
            " |          This reference table is for the v2 version of the schema.\n",
            " |\n",
            " |      | event                  | name                 | chunk                               | input                                             | output                                              |\n",
            " |      | ---------------------- | -------------------- | ----------------------------------- | ------------------------------------------------- | --------------------------------------------------- |\n",
            " |      | `on_chat_model_start`  | `'[model name]'`     |                                     | `{\"messages\": [[SystemMessage, HumanMessage]]}`   |                                                     |\n",
            " |      | `on_chat_model_stream` | `'[model name]'`     | `AIMessageChunk(content=\"hello\")`   |                                                   |                                                     |\n",
            " |      | `on_chat_model_end`    | `'[model name]'`     |                                     | `{\"messages\": [[SystemMessage, HumanMessage]]}`   | `AIMessageChunk(content=\"hello world\")`             |\n",
            " |      | `on_llm_start`         | `'[model name]'`     |                                     | `{'input': 'hello'}`                              |                                                     |\n",
            " |      | `on_llm_stream`        | `'[model name]'`     | `'Hello' `                          |                                                   |                                                     |\n",
            " |      | `on_llm_end`           | `'[model name]'`     |                                     | `'Hello human!'`                                  |                                                     |\n",
            " |      | `on_chain_start`       | `'format_docs'`      |                                     |                                                   |                                                     |\n",
            " |      | `on_chain_stream`      | `'format_docs'`      | `'hello world!, goodbye world!'`    |                                                   |                                                     |\n",
            " |      | `on_chain_end`         | `'format_docs'`      |                                     | `[Document(...)]`                                 | `'hello world!, goodbye world!'`                    |\n",
            " |      | `on_tool_start`        | `'some_tool'`        |                                     | `{\"x\": 1, \"y\": \"2\"}`                              |                                                     |\n",
            " |      | `on_tool_end`          | `'some_tool'`        |                                     |                                                   | `{\"x\": 1, \"y\": \"2\"}`                                |\n",
            " |      | `on_retriever_start`   | `'[retriever name]'` |                                     | `{\"query\": \"hello\"}`                              |                                                     |\n",
            " |      | `on_retriever_end`     | `'[retriever name]'` |                                     | `{\"query\": \"hello\"}`                              | `[Document(...), ..]`                               |\n",
            " |      | `on_prompt_start`      | `'[template_name]'`  |                                     | `{\"question\": \"hello\"}`                           |                                                     |\n",
            " |      | `on_prompt_end`        | `'[template_name]'`  |                                     | `{\"question\": \"hello\"}`                           | `ChatPromptValue(messages: [SystemMessage, ...])`   |\n",
            " |\n",
            " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
            " |\n",
            " |      Custom events will be only be surfaced with in the v2 version of the API!\n",
            " |\n",
            " |      A custom event has following format:\n",
            " |\n",
            " |      | Attribute   | Type   | Description                                                                                               |\n",
            " |      | ----------- | ------ | --------------------------------------------------------------------------------------------------------- |\n",
            " |      | `name`      | `str`  | A user defined name for the event.                                                                        |\n",
            " |      | `data`      | `Any`  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
            " |\n",
            " |      Here are declarations associated with the standard events shown above:\n",
            " |\n",
            " |      `format_docs`:\n",
            " |\n",
            " |      ```python\n",
            " |      def format_docs(docs: list[Document]) -> str:\n",
            " |          '''Format the docs.'''\n",
            " |          return \", \".join([doc.page_content for doc in docs])\n",
            " |\n",
            " |\n",
            " |      format_docs = RunnableLambda(format_docs)\n",
            " |      ```\n",
            " |\n",
            " |      `some_tool`:\n",
            " |\n",
            " |      ```python\n",
            " |      @tool\n",
            " |      def some_tool(x: int, y: str) -> dict:\n",
            " |          '''Some_tool.'''\n",
            " |          return {\"x\": x, \"y\": y}\n",
            " |      ```\n",
            " |\n",
            " |      `prompt`:\n",
            " |\n",
            " |      ```python\n",
            " |      template = ChatPromptTemplate.from_messages(\n",
            " |          [\n",
            " |              (\"system\", \"You are Cat Agent 007\"),\n",
            " |              (\"human\", \"{question}\"),\n",
            " |          ]\n",
            " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
            " |      ```\n",
            " |\n",
            " |      !!! example\n",
            " |\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          async def reverse(s: str) -> str:\n",
            " |              return s[::-1]\n",
            " |\n",
            " |\n",
            " |          chain = RunnableLambda(func=reverse)\n",
            " |\n",
            " |          events = [\n",
            " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
            " |          ]\n",
            " |\n",
            " |          # Will produce the following events\n",
            " |          # (run_id, and parent_ids has been omitted for brevity):\n",
            " |          [\n",
            " |              {\n",
            " |                  \"data\": {\"input\": \"hello\"},\n",
            " |                  \"event\": \"on_chain_start\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"chunk\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_stream\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"output\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_end\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |          ]\n",
            " |          ```\n",
            " |\n",
            " |      ```python title=\"Dispatch custom event\"\n",
            " |      from langchain_core.callbacks.manager import (\n",
            " |          adispatch_custom_event,\n",
            " |      )\n",
            " |      from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
            " |      import asyncio\n",
            " |\n",
            " |\n",
            " |      async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
            " |          \"\"\"Do something that takes a long time.\"\"\"\n",
            " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
            " |          await adispatch_custom_event(\n",
            " |              \"progress_event\",\n",
            " |              {\"message\": \"Finished step 1 of 3\"},\n",
            " |              config=config # Must be included for python < 3.10\n",
            " |          )\n",
            " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
            " |          await adispatch_custom_event(\n",
            " |              \"progress_event\",\n",
            " |              {\"message\": \"Finished step 2 of 3\"},\n",
            " |              config=config # Must be included for python < 3.10\n",
            " |          )\n",
            " |          await asyncio.sleep(1) # Placeholder for some slow operation\n",
            " |          return \"Done\"\n",
            " |\n",
            " |      slow_thing = RunnableLambda(slow_thing)\n",
            " |\n",
            " |      async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
            " |          print(event)\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the `Runnable`.\n",
            " |          config: The config to use for the `Runnable`.\n",
            " |          version: The version of the schema to use, either `'v2'` or `'v1'`.\n",
            " |\n",
            " |              Users should use `'v2'`.\n",
            " |\n",
            " |              `'v1'` is for backwards compatibility and will be deprecated\n",
            " |              in `0.4.0`.\n",
            " |\n",
            " |              No default will be assigned until the API is stabilized.\n",
            " |              custom events will only be surfaced in `'v2'`.\n",
            " |          include_names: Only include events from `Runnable` objects with matching names.\n",
            " |          include_types: Only include events from `Runnable` objects with matching types.\n",
            " |          include_tags: Only include events from `Runnable` objects with matching tags.\n",
            " |          exclude_names: Exclude events from `Runnable` objects with matching names.\n",
            " |          exclude_types: Exclude events from `Runnable` objects with matching types.\n",
            " |          exclude_tags: Exclude events from `Runnable` objects with matching tags.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |              These will be passed to `astream_log` as this implementation\n",
            " |              of `astream_events` is built on top of `astream_log`.\n",
            " |\n",
            " |      Yields:\n",
            " |          An async stream of `StreamEvent`.\n",
            " |\n",
            " |      Raises:\n",
            " |          NotImplementedError: If the version is not `'v1'` or `'v2'`.\n",
            " |\n",
            " |  async astream_log(self, input: 'Any', config: 'RunnableConfig | None' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Sequence[str] | None' = None, include_types: 'Sequence[str] | None' = None, include_tags: 'Sequence[str] | None' = None, exclude_names: 'Sequence[str] | None' = None, exclude_types: 'Sequence[str] | None' = None, exclude_tags: 'Sequence[str] | None' = None, **kwargs: 'Any') -> 'AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]'\n",
            " |      Stream all output from a `Runnable`, as reported to the callback system.\n",
            " |\n",
            " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
            " |\n",
            " |      Output is streamed as Log objects, which include a list of\n",
            " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
            " |      step, and the final state of the run.\n",
            " |\n",
            " |      The Jsonpatch ops can be applied in order to construct state.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the `Runnable`.\n",
            " |          config: The config to use for the `Runnable`.\n",
            " |          diff: Whether to yield diffs between each step or the current state.\n",
            " |          with_streamed_output_list: Whether to yield the `streamed_output` list.\n",
            " |          include_names: Only include logs with these names.\n",
            " |          include_types: Only include logs with these types.\n",
            " |          include_tags: Only include logs with these tags.\n",
            " |          exclude_names: Exclude logs with these names.\n",
            " |          exclude_types: Exclude logs with these types.\n",
            " |          exclude_tags: Exclude logs with these tags.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          A `RunLogPatch` or `RunLog` object.\n",
            " |\n",
            " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'RunnableConfig | None' = None, **kwargs: 'Any | None') -> 'AsyncIterator[Output]'\n",
            " |      Transform inputs to outputs.\n",
            " |\n",
            " |      Default implementation of atransform, which buffers input and calls `astream`.\n",
            " |\n",
            " |      Subclasses must override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |\n",
            " |      Args:\n",
            " |          input: An async iterator of inputs to the `Runnable`.\n",
            " |          config: The config to use for the `Runnable`.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the `Runnable`.\n",
            " |\n",
            " |  batch(self, inputs: 'list[Input]', config: 'RunnableConfig | list[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'list[Output]'\n",
            " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
            " |\n",
            " |      The default implementation of batch works well for IO bound runnables.\n",
            " |\n",
            " |      Subclasses must override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying `Runnable` uses an API which supports a batch mode.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the `Runnable`.\n",
            " |          config: A config to use when invoking the `Runnable`. The config supports\n",
            " |              standard keys like `'tags'`, `'metadata'` for\n",
            " |              tracing purposes, `'max_concurrency'` for controlling how much work\n",
            " |              to do in parallel, and other keys.\n",
            " |\n",
            " |              Please refer to `RunnableConfig` for more details.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of outputs from the `Runnable`.\n",
            " |\n",
            " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'RunnableConfig | Sequence[RunnableConfig] | None' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any | None') -> 'Iterator[tuple[int, Output | Exception]]'\n",
            " |      Run `invoke` in parallel on a list of inputs.\n",
            " |\n",
            " |      Yields results as they complete.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the `Runnable`.\n",
            " |          config: A config to use when invoking the `Runnable`.\n",
            " |\n",
            " |              The config supports standard keys like `'tags'`, `'metadata'` for\n",
            " |              tracing purposes, `'max_concurrency'` for controlling how much work to\n",
            " |              do in parallel, and other keys.\n",
            " |\n",
            " |              Please refer to `RunnableConfig` for more details.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          Tuples of the index of the input and the output from the `Runnable`.\n",
            " |\n",
            " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind arguments to a `Runnable`, returning a new `Runnable`.\n",
            " |\n",
            " |      Useful when a `Runnable` in a chain requires an argument that is not\n",
            " |      in the output of the previous `Runnable` or included in the user input.\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: The arguments to bind to the `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the arguments bound.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_ollama import ChatOllama\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |\n",
            " |          model = ChatOllama(model=\"llama3.1\")\n",
            " |\n",
            " |          # Without bind\n",
            " |          chain = model | StrOutputParser()\n",
            " |\n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two three four five.'\n",
            " |\n",
            " |          # With bind\n",
            " |          chain = model.bind(stop=[\"three\"]) | StrOutputParser()\n",
            " |\n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two'\n",
            " |          ```\n",
            " |\n",
            " |  config_schema(self, *, include: 'Sequence[str] | None' = None) -> 'type[BaseModel]'\n",
            " |      The type of config this `Runnable` accepts specified as a Pydantic model.\n",
            " |\n",
            " |      To mark a field as configurable, see the `configurable_fields`\n",
            " |      and `configurable_alternatives` methods.\n",
            " |\n",
            " |      Args:\n",
            " |          include: A list of fields to include in the config schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A Pydantic model that can be used to validate config.\n",
            " |\n",
            " |  get_config_jsonschema(self, *, include: 'Sequence[str] | None' = None) -> 'dict[str, Any]'\n",
            " |      Get a JSON schema that represents the config of the `Runnable`.\n",
            " |\n",
            " |      Args:\n",
            " |          include: A list of fields to include in the config schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema that represents the config of the `Runnable`.\n",
            " |\n",
            " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
            " |\n",
            " |  get_graph(self, config: 'RunnableConfig | None' = None) -> 'Graph'\n",
            " |      Return a graph representation of this `Runnable`.\n",
            " |\n",
            " |  get_input_jsonschema(self, config: 'RunnableConfig | None' = None) -> 'dict[str, Any]'\n",
            " |      Get a JSON schema that represents the input to the `Runnable`.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema that represents the input to the `Runnable`.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def add_one(x: int) -> int:\n",
            " |              return x + 1\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(add_one)\n",
            " |\n",
            " |          print(runnable.get_input_jsonschema())\n",
            " |          ```\n",
            " |\n",
            " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
            " |\n",
            " |  get_input_schema(self, config: 'RunnableConfig | None' = None) -> 'type[BaseModel]'\n",
            " |      Get a Pydantic model that can be used to validate input to the `Runnable`.\n",
            " |\n",
            " |      `Runnable` objects that leverage the `configurable_fields` and\n",
            " |      `configurable_alternatives` methods will have a dynamic input schema that\n",
            " |      depends on which configuration the `Runnable` is invoked with.\n",
            " |\n",
            " |      This method allows to get an input schema for a specific configuration.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A Pydantic model that can be used to validate input.\n",
            " |\n",
            " |  get_name(self, suffix: 'str | None' = None, *, name: 'str | None' = None) -> 'str'\n",
            " |      Get the name of the `Runnable`.\n",
            " |\n",
            " |      Args:\n",
            " |          suffix: An optional suffix to append to the name.\n",
            " |          name: An optional name to use instead of the `Runnable`'s name.\n",
            " |\n",
            " |      Returns:\n",
            " |          The name of the `Runnable`.\n",
            " |\n",
            " |  get_output_jsonschema(self, config: 'RunnableConfig | None' = None) -> 'dict[str, Any]'\n",
            " |      Get a JSON schema that represents the output of the `Runnable`.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema that represents the output of the `Runnable`.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def add_one(x: int) -> int:\n",
            " |              return x + 1\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(add_one)\n",
            " |\n",
            " |          print(runnable.get_output_jsonschema())\n",
            " |          ```\n",
            " |\n",
            " |      !!! version-added \"Added in `langchain-core` 0.3.0\"\n",
            " |\n",
            " |  get_output_schema(self, config: 'RunnableConfig | None' = None) -> 'type[BaseModel]'\n",
            " |      Get a Pydantic model that can be used to validate output to the `Runnable`.\n",
            " |\n",
            " |      `Runnable` objects that leverage the `configurable_fields` and\n",
            " |      `configurable_alternatives` methods will have a dynamic output schema that\n",
            " |      depends on which configuration the `Runnable` is invoked with.\n",
            " |\n",
            " |      This method allows to get an output schema for a specific configuration.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A Pydantic model that can be used to validate output.\n",
            " |\n",
            " |  get_prompts(self, config: 'RunnableConfig | None' = None) -> 'list[BasePromptTemplate]'\n",
            " |      Return a list of prompts used by this `Runnable`.\n",
            " |\n",
            " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
            " |      Return a new `Runnable` that maps a list of inputs to a list of outputs.\n",
            " |\n",
            " |      Calls `invoke` with each input.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` that maps a list of inputs to a list of outputs.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def _lambda(x: int) -> int:\n",
            " |              return x + 1\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(_lambda)\n",
            " |          print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]\n",
            " |          ```\n",
            " |\n",
            " |  pick(self, keys: 'str | list[str]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Pick keys from the output `dict` of this `Runnable`.\n",
            " |\n",
            " |      !!! example \"Pick a single key\"\n",
            " |\n",
            " |          ```python\n",
            " |          import json\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |\n",
            " |          as_str = RunnableLambda(str)\n",
            " |          as_json = RunnableLambda(json.loads)\n",
            " |          chain = RunnableMap(str=as_str, json=as_json)\n",
            " |\n",
            " |          chain.invoke(\"[1, 2, 3]\")\n",
            " |          # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
            " |\n",
            " |          json_only_chain = chain.pick(\"json\")\n",
            " |          json_only_chain.invoke(\"[1, 2, 3]\")\n",
            " |          # -> [1, 2, 3]\n",
            " |          ```\n",
            " |\n",
            " |      !!! example \"Pick a list of keys\"\n",
            " |\n",
            " |          ```python\n",
            " |          from typing import Any\n",
            " |\n",
            " |          import json\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |\n",
            " |          as_str = RunnableLambda(str)\n",
            " |          as_json = RunnableLambda(json.loads)\n",
            " |\n",
            " |\n",
            " |          def as_bytes(x: Any) -> bytes:\n",
            " |              return bytes(x, \"utf-8\")\n",
            " |\n",
            " |\n",
            " |          chain = RunnableMap(\n",
            " |              str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)\n",
            " |          )\n",
            " |\n",
            " |          chain.invoke(\"[1, 2, 3]\")\n",
            " |          # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |\n",
            " |          json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
            " |          json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
            " |          # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |          ```\n",
            " |\n",
            " |      Args:\n",
            " |          keys: A key or list of keys to pick from the output dict.\n",
            " |\n",
            " |      Returns:\n",
            " |          a new `Runnable`.\n",
            " |\n",
            " |  pipe(self, *others: 'Runnable[Any, Other] | Callable[[Any], Other]', name: 'str | None' = None) -> 'RunnableSerializable[Input, Other]'\n",
            " |      Pipe `Runnable` objects.\n",
            " |\n",
            " |      Compose this `Runnable` with `Runnable`-like objects to make a\n",
            " |      `RunnableSequence`.\n",
            " |\n",
            " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def add_one(x: int) -> int:\n",
            " |              return x + 1\n",
            " |\n",
            " |\n",
            " |          def mul_two(x: int) -> int:\n",
            " |              return x * 2\n",
            " |\n",
            " |\n",
            " |          runnable_1 = RunnableLambda(add_one)\n",
            " |          runnable_2 = RunnableLambda(mul_two)\n",
            " |          sequence = runnable_1.pipe(runnable_2)\n",
            " |          # Or equivalently:\n",
            " |          # sequence = runnable_1 | runnable_2\n",
            " |          # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
            " |          sequence.invoke(1)\n",
            " |          await sequence.ainvoke(1)\n",
            " |          # -> 4\n",
            " |\n",
            " |          sequence.batch([1, 2, 3])\n",
            " |          await sequence.abatch([1, 2, 3])\n",
            " |          # -> [4, 6, 8]\n",
            " |          ```\n",
            " |\n",
            " |      Args:\n",
            " |          *others: Other `Runnable` or `Runnable`-like objects to compose\n",
            " |          name: An optional name for the resulting `RunnableSequence`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable`.\n",
            " |\n",
            " |  transform(self, input: 'Iterator[Input]', config: 'RunnableConfig | None' = None, **kwargs: 'Any | None') -> 'Iterator[Output]'\n",
            " |      Transform inputs to outputs.\n",
            " |\n",
            " |      Default implementation of transform, which buffers input and calls `astream`.\n",
            " |\n",
            " |      Subclasses must override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |\n",
            " |      Args:\n",
            " |          input: An iterator of inputs to the `Runnable`.\n",
            " |          config: The config to use for the `Runnable`.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the `Runnable`.\n",
            " |\n",
            " |  with_alisteners(self, *, on_start: 'AsyncListener | None' = None, on_end: 'AsyncListener | None' = None, on_error: 'AsyncListener | None' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind async lifecycle listeners to a `Runnable`.\n",
            " |\n",
            " |      Returns a new `Runnable`.\n",
            " |\n",
            " |      The Run object contains information about the run, including its `id`,\n",
            " |      `type`, `input`, `output`, `error`, `start_time`, `end_time`, and\n",
            " |      any tags or metadata added to the run.\n",
            " |\n",
            " |      Args:\n",
            " |          on_start: Called asynchronously before the `Runnable` starts running,\n",
            " |              with the `Run` object.\n",
            " |          on_end: Called asynchronously after the `Runnable` finishes running,\n",
            " |              with the `Run` object.\n",
            " |          on_error: Called asynchronously if the `Runnable` throws an error,\n",
            " |              with the `Run` object.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the listeners bound.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda, Runnable\n",
            " |          from datetime import datetime, timezone\n",
            " |          import time\n",
            " |          import asyncio\n",
            " |\n",
            " |\n",
            " |          def format_t(timestamp: float) -> str:\n",
            " |              return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
            " |\n",
            " |\n",
            " |          async def test_runnable(time_to_sleep: int):\n",
            " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
            " |              await asyncio.sleep(time_to_sleep)\n",
            " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
            " |\n",
            " |\n",
            " |          async def fn_start(run_obj: Runnable):\n",
            " |              print(f\"on start callback starts at {format_t(time.time())}\")\n",
            " |              await asyncio.sleep(3)\n",
            " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
            " |\n",
            " |\n",
            " |          async def fn_end(run_obj: Runnable):\n",
            " |              print(f\"on end callback starts at {format_t(time.time())}\")\n",
            " |              await asyncio.sleep(2)\n",
            " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
            " |              on_start=fn_start, on_end=fn_end\n",
            " |          )\n",
            " |\n",
            " |\n",
            " |          async def concurrent_runs():\n",
            " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
            " |\n",
            " |\n",
            " |          asyncio.run(concurrent_runs())\n",
            " |          # Result:\n",
            " |          # on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
            " |          # on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
            " |          # on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
            " |          # on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
            " |          # Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
            " |          # Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
            " |          # Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
            " |          # on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
            " |          # Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
            " |          # on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
            " |          # on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
            " |          # on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
            " |          ```\n",
            " |\n",
            " |  with_config(self, config: 'RunnableConfig | None' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind config to a `Runnable`, returning a new `Runnable`.\n",
            " |\n",
            " |      Args:\n",
            " |          config: The config to bind to the `Runnable`.\n",
            " |          **kwargs: Additional keyword arguments to pass to the `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the config bound.\n",
            " |\n",
            " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'str | None' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
            " |      Add fallbacks to a `Runnable`, returning a new `Runnable`.\n",
            " |\n",
            " |      The new `Runnable` will try the original `Runnable`, and then each fallback\n",
            " |      in order, upon failures.\n",
            " |\n",
            " |      Args:\n",
            " |          fallbacks: A sequence of runnables to try if the original `Runnable`\n",
            " |              fails.\n",
            " |          exceptions_to_handle: A tuple of exception types to handle.\n",
            " |          exception_key: If `string` is specified then handled exceptions will be\n",
            " |              passed to fallbacks as part of the input under the specified key.\n",
            " |\n",
            " |              If `None`, exceptions will not be passed to fallbacks.\n",
            " |\n",
            " |              If used, the base `Runnable` and its fallbacks must accept a\n",
            " |              dictionary as input.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` that will try the original `Runnable`, and then each\n",
            " |              Fallback in order, upon failures.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from typing import Iterator\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableGenerator\n",
            " |\n",
            " |\n",
            " |          def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
            " |              raise ValueError()\n",
            " |              yield \"\"\n",
            " |\n",
            " |\n",
            " |          def _generate(input: Iterator) -> Iterator[str]:\n",
            " |              yield from \"foo bar\"\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
            " |              [RunnableGenerator(_generate)]\n",
            " |          )\n",
            " |          print(\"\".join(runnable.stream({})))  # foo bar\n",
            " |          ```\n",
            " |\n",
            " |      Args:\n",
            " |          fallbacks: A sequence of runnables to try if the original `Runnable`\n",
            " |              fails.\n",
            " |          exceptions_to_handle: A tuple of exception types to handle.\n",
            " |          exception_key: If `string` is specified then handled exceptions will be\n",
            " |              passed to fallbacks as part of the input under the specified key.\n",
            " |\n",
            " |              If `None`, exceptions will not be passed to fallbacks.\n",
            " |\n",
            " |              If used, the base `Runnable` and its fallbacks must accept a\n",
            " |              dictionary as input.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` that will try the original `Runnable`, and then each\n",
            " |              Fallback in order, upon failures.\n",
            " |\n",
            " |  with_listeners(self, *, on_start: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None, on_end: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None, on_error: 'Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind lifecycle listeners to a `Runnable`, returning a new `Runnable`.\n",
            " |\n",
            " |      The Run object contains information about the run, including its `id`,\n",
            " |      `type`, `input`, `output`, `error`, `start_time`, `end_time`, and\n",
            " |      any tags or metadata added to the run.\n",
            " |\n",
            " |      Args:\n",
            " |          on_start: Called before the `Runnable` starts running, with the `Run`\n",
            " |              object.\n",
            " |          on_end: Called after the `Runnable` finishes running, with the `Run`\n",
            " |              object.\n",
            " |          on_error: Called if the `Runnable` throws an error, with the `Run`\n",
            " |              object.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the listeners bound.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |          from langchain_core.tracers.schemas import Run\n",
            " |\n",
            " |          import time\n",
            " |\n",
            " |\n",
            " |          def test_runnable(time_to_sleep: int):\n",
            " |              time.sleep(time_to_sleep)\n",
            " |\n",
            " |\n",
            " |          def fn_start(run_obj: Run):\n",
            " |              print(\"start_time:\", run_obj.start_time)\n",
            " |\n",
            " |\n",
            " |          def fn_end(run_obj: Run):\n",
            " |              print(\"end_time:\", run_obj.end_time)\n",
            " |\n",
            " |\n",
            " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
            " |              on_start=fn_start, on_end=fn_end\n",
            " |          )\n",
            " |          chain.invoke(2)\n",
            " |          ```\n",
            " |\n",
            " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, exponential_jitter_params: 'ExponentialJitterParams | None' = None, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
            " |      Create a new `Runnable` that retries the original `Runnable` on exceptions.\n",
            " |\n",
            " |      Args:\n",
            " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
            " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
            " |              time between retries.\n",
            " |          stop_after_attempt: The maximum number of attempts to make before\n",
            " |              giving up.\n",
            " |          exponential_jitter_params: Parameters for\n",
            " |              `tenacity.wait_exponential_jitter`. Namely: `initial`, `max`,\n",
            " |              `exp_base`, and `jitter` (all `float` values).\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` that retries the original `Runnable` on exceptions.\n",
            " |\n",
            " |      Example:\n",
            " |          ```python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |          count = 0\n",
            " |\n",
            " |\n",
            " |          def _lambda(x: int) -> None:\n",
            " |              global count\n",
            " |              count = count + 1\n",
            " |              if x == 1:\n",
            " |                  raise ValueError(\"x is 1\")\n",
            " |              else:\n",
            " |                  pass\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(_lambda)\n",
            " |          try:\n",
            " |              runnable.with_retry(\n",
            " |                  stop_after_attempt=2,\n",
            " |                  retry_if_exception_type=(ValueError,),\n",
            " |              ).invoke(1)\n",
            " |          except ValueError:\n",
            " |              pass\n",
            " |\n",
            " |          assert count == 2\n",
            " |          ```\n",
            " |\n",
            " |  with_types(self, *, input_type: 'type[Input] | None' = None, output_type: 'type[Output] | None' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind input and output types to a `Runnable`, returning a new `Runnable`.\n",
            " |\n",
            " |      Args:\n",
            " |          input_type: The input type to bind to the `Runnable`.\n",
            " |          output_type: The output type to bind to the `Runnable`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new `Runnable` with the types bound.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
            " |\n",
            " |  config_specs\n",
            " |      List configurable fields for this `Runnable`.\n",
            " |\n",
            " |  input_schema\n",
            " |      The type of input this `Runnable` accepts specified as a Pydantic model.\n",
            " |\n",
            " |  output_schema\n",
            " |      Output schema.\n",
            " |\n",
            " |      The type of output this `Runnable` produces specified as a Pydantic model.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |\n",
            " |  __init_subclass__(...)\n",
            " |      Function to initialize subclasses.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "help(ChatGoogleGenerativeAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPiJiremScD5"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F08gJvPoQhX"
      },
      "source": [
        "#### First Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N2YE7g0UVqd",
        "outputId": "39397048-6317-4640-f8ac-05cb24d60f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ayushman Bharat is a flagship national health protection scheme launched by the Government of India. It aims to achieve Universal Health Coverage (UHC) and provide accessible, affordable, and quality healthcare services to the citizens of India.\n",
            "\n",
            "The scheme has two main components:\n",
            "\n",
            "1.  **Pradhan Mantri Jan Arogya Yojana (PMJAY):** This is the world's largest government-funded health assurance scheme.\n",
            "    *   **Objective:** To provide health insurance coverage to the poorest and most vulnerable families.\n",
            "    *   **Coverage:** It offers a health cover of up to INR 5 lakh (approximately USD 6,000) per family per year for secondary and tertiary care hospitalization across public and empanelled private hospitals.\n",
            "    *   **Beneficiaries:** It covers over 10.74 crore (107.4 million) poor and vulnerable families (approximately 50 crore or 500 million beneficiaries) based on the Socio-Economic Caste Census (SECC) data.\n",
            "    *   **Nature:** It is a cashless and paperless scheme, meaning beneficiaries do not have to pay anything out-of-pocket for covered treatments.\n",
            "\n",
            "2.  **Ayushman Bharat Health and Wellness Centres (AB-HWCs):** These centres are designed to transform existing Primary Health Centres (PHCs) and Sub-Centres into Health and Wellness Centres.\n",
            "    *   **Objective:** To provide comprehensive primary healthcare, bringing healthcare closer to the homes of people.\n",
            "    *   **Services:** They offer a wide range of services, including maternal and child health services, non-communicable disease screening and management, free essential drugs and diagnostic services, geriatric care, palliative care, and mental health services.\n",
            "    *   **Focus:** They emphasize preventive and promotive health, along with curative care, to reduce the burden of disease and out-of-pocket expenditure.\n",
            "\n",
            "In essence, Ayushman Bharat is a comprehensive initiative to strengthen India's healthcare system by providing both health insurance for hospital care (PMJAY) and robust primary healthcare services at the community level (AB-HWCs).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 1. Initialize the Chat Model (from the previous step)\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Prepare the messages\n",
        "# The SystemMessage sets the behavioral context for the AI.\n",
        "# The HumanMessage is the user's actual query.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"What is Ayushman Bharat?\"),\n",
        "]\n",
        "\n",
        "# 3. Invoke the model with the messages\n",
        "result = chat_model.invoke(messages) # notice the similarity with model.predict from sklearn\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzzuvFGqUn0k",
        "outputId": "1a21ab1a-e07b-4743-fe5f-60d3a600e13b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Blood pressure is the **force of your blood pushing against the walls of your arteries** as your heart pumps it throughout your body.\\n\\nThink of it like water flowing through a garden hose: the pressure inside the hose is what keeps the water moving. Similarly, blood pressure is essential for circulating blood, oxygen, and nutrients to all your organs and tissues.\\n\\nIt\\'s measured in **millimeters of mercury (mmHg)** and is expressed as two numbers:\\n\\n1.  **Systolic Pressure (the top number):** This is the higher number and represents the pressure in your arteries when your heart **beats** and pushes blood out.\\n2.  **Diastolic Pressure (the bottom number):** This is the lower number and represents the pressure in your arteries when your heart is **at rest** between beats.\\n\\nFor example, a blood pressure reading of \"120/80 mmHg\" means you have a systolic pressure of 120 and a diastolic pressure of 80.\\n\\n**Why is it important?**\\n\\n*   **Too high (Hypertension):** Consistently high blood pressure can damage your arteries over time, making them less elastic and increasing your risk of serious health problems like heart attack, stroke, kidney disease, and vision loss. It\\'s often called the \"silent killer\" because it usually has no symptoms.\\n*   **Too low (Hypotension):** Abnormally low blood pressure can mean that not enough blood is reaching your organs, leading to symptoms like dizziness, lightheadedness, fainting, and in severe cases, organ damage.\\n\\nRegular monitoring of blood pressure is a key part of maintaining good health.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bcca2-014d-77c2-9b3d-7cc78097ef7c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 6, 'output_tokens': 1231, 'total_tokens': 1237, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 891}})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_model.invoke(\"What is blood pressure?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmD3qL7_oTGd"
      },
      "source": [
        "#### Second Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_eeP53XUFLr",
        "outputId": "94876599-eb0f-4989-fa1c-304c004734bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I can only answer healthcare-related questions. Changing a tire is not a healthcare topic.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Prepare messages with an out-of-scope question\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"How do I change a tire?\"),\n",
        "]\n",
        "\n",
        "# 3. Invoke the model\n",
        "result = chat_model.invoke(messages)\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNAWRLXoTePQ",
        "outputId": "59085641-f568-4ed4-924f-a148e2c2fda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, someone had a positive experience. They described the discharge process as \"seamless.\"\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Create the Prompt Template\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions. Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "review_template = ChatPromptTemplate.from_template(instruction_str)\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"The discharge process was seamless!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain by piping the components together\n",
        "#    We also add an output parser to get a clean string result.\n",
        "chain = review_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain with the input variables\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRQwJ18ZZCuT"
      },
      "source": [
        "## Chains\n",
        "\n",
        "The chaining process:\n",
        " - `review_prompt_template`: Generates a formatted prompt using the input variables (e.g., `context` and `question`).\n",
        " - `chat_model`: Sends the formatted prompt to the specified Gemini chat model for generating a response.\n",
        " - `output_parser`: Processes the raw output from the chat model and ensures it is returned as a clean string.\n",
        "\n",
        " The `|` operator acts as a pipeline, automatically passing the output of one step (e.g., formatted prompt)  as the input to the next (e.g., chat model and then output parser). This simplifies the workflow for generating responses from the chat model with structured input and output handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--bZZ51YYb0s"
      },
      "source": [
        "## Using PromptTemplates & MessageTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO3lbtv8P8wG"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "prompt = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompttemplate=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=instruction_str\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhrsnDNPTeMa",
        "outputId": "4f439e9e-3d11-448d-d961-bef7cd3e38db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, one patient stated, \"I had a great stay!\"\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Create the detailed prompt templates\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=instruction_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"], template=\"{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# This is our final, reusable prompt template\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"I had a great stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain\n",
        "chain = review_prompt_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "hiKXXNE9Td0i",
        "outputId": "486d337c-226e-4cfc-8707-0d3a17b98a9d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Based on the provided context, I only have information about one person who had a negative stay. I don't have any information about anyone having a positive experience.\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = \"I had a negative stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "chain.invoke({\"context\": context, \"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KH3JCq5ra_2"
      },
      "source": [
        "# Adding RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "OD9upnVJkxDV",
        "outputId": "7ae788fb-106a-4edf-8404-473090cf59e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-1.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting chromadb<2.0.0,>=1.3.5 (from langchain-chroma)\n",
            "  Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (1.2.7)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Collecting build>=1.0.3 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.12.3)\n",
            "Collecting pybase64>=1.4.1 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.40.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.22.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading pypika-0.50.0-py2.py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.21.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading kubernetes-35.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.26.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (25.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-chroma) (0.13.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.32.5)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-chroma) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.4.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.3.0)\n",
            "Downloading langchain_chroma-1.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading kubernetes-35.0.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypika-0.50.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypika, durationpy, pyproject_hooks, pybase64, opentelemetry-proto, humanfriendly, bcrypt, backoff, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb, langchain-chroma\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.4.0 chromadb-1.4.1 coloredlogs-15.0.1 durationpy-0.10 humanfriendly-10.0 kubernetes-35.0.0 langchain-chroma-1.1.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.50.0 pyproject_hooks-1.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "74067c1a23bd49ba8f1bd5d0a78afa96",
              "pip_warning": {
                "packages": [
                  "opentelemetry"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -U langchain-chroma # FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "8vrBwvI9YwTr",
        "outputId": "cdb10f95-8c21-44dc-a562-ed3e4d3a295a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1/51...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'add_documents'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-65952386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# For subsequent batches, add the documents to the existing database.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mreviews_vector_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Pause the script for 30 seconds after each batch to respect the per-minute rate limit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'add_documents'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Import the CSVLoader class to load documents from a CSV file.\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "# Import the Chroma class, which is used to create and interact with a Chroma vector database.\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Import the GoogleGenerativeAIEmbeddings class to create numerical vector representations (embeddings) of text using Google's models.\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "\n",
        "REVIEWS_CSV_PATH = \"/content/drive/MyDrive/0. GFG/Courses/21 Projects in 21 Days/18. Chat with Your Knowledge Base: Building a Powerful RAG Chatbot/reviews.csv\"\n",
        "\n",
        "# Define a constant variable for the directory where the Chroma vector database will be stored.\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
        "\n",
        "\n",
        "# Create an instance of the CSVLoader.\n",
        "loader = CSVLoader(\n",
        "    file_path=REVIEWS_CSV_PATH,  # Specify the path to the CSV file to be loaded.\n",
        "    source_column=\"review\"       # Specify the name of the column that contains the main text content.\n",
        ")\n",
        "\n",
        "# Call the .load() method on the loader instance.\n",
        "# This reads the specified column from the CSV file and loads the content into a list of Document objects.\n",
        "reviews = loader.load()\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process.\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches.\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n",
        "\n",
        "# Loop through the documents in batches to avoid hitting the API's rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    # Get the current batch of documents.\n",
        "    batch_docs = reviews[i:i + batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num}/{num_batches}...\")\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first batch, create a new Chroma vector database.\n",
        "        # The `from_documents` method handles the entire process of embedding and storing the data.\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,  # Pass the list of Document objects that need to be embedded.\n",
        "            embedding=embedding_function,\n",
        "            # Specify the directory on the disk where the vector database will be saved.\n",
        "            # This makes the database persistent, so we can load it directly in the future.\n",
        "            persist_directory=REVIEWS_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        # For subsequent batches, add the documents to the existing database.\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 30 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0082YQZi7dG"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVYcN9PmV8xA",
        "outputId": "abd0abb9-9411-4f1d-fa52-de5ce64b1f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method similarity_search in module langchain_chroma.vectorstores:\n",
            "\n",
            "similarity_search(query: 'str', k: 'int' = 4, filter: 'dict[str, str] | None' = None, **kwargs: 'Any') -> 'list[Document]' method of langchain_chroma.vectorstores.Chroma instance\n",
            "    Run similarity search with Chroma.\n",
            "\n",
            "    Args:\n",
            "        query: Query text to search for.\n",
            "        k: Number of results to return.\n",
            "        filter: Filter by metadata.\n",
            "        kwargs: Additional keyword arguments to pass to Chroma collection query.\n",
            "\n",
            "    Returns:\n",
            "        List of documents most similar to the query text.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(reviews_vector_db.similarity_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "FEKxBKTNi7LZ",
        "outputId": "d6df91d6-7647-43f7-944a-8165778c4710"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'review_id: 43\\nvisit_id: 2353\\nreview: The hospital staff lacked proper communication among themselves, leading to confusion about my treatment plan. Clear and cohesive communication is essential for patient care and overall satisfaction.\\nphysician_name: Erika Ingram\\nhospital_name: Shea LLC\\npatient_name: Dennis Fitzgerald'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "relevant_info = reviews_vector_db.similarity_search(question, k=3)\n",
        "\n",
        "relevant_info[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qrmyucMmffPJ",
        "outputId": "8aeb60f1-07dd-41c9-fb3f-761d06900572"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'review_id: 773\\nvisit_id: 6383\\nreview: I encountered some communication issues during my stay. The medical staff seemed disorganized, and it led to confusion about my treatment plan.\\nphysician_name: Jacqueline Johnson\\nhospital_name: Rush, Owens and Johnson\\npatient_name: Jessica Mays'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevant_info[1].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "NeYvAtnSjATn",
        "outputId": "d7813fad-4f78-43f1-db99-e33aa9a250aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"review_id: 707\\nvisit_id: 4533\\nreview: I encountered some issues with the nursing staff's communication. It seemed like there was a lack of coordination, leading to confusion about my medication schedule and treatment plan.\\nphysician_name: Joseph Gonzales\\nhospital_name: Brown-Golden\\npatient_name: Makayla Reynolds\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevant_info[2].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_-CkirwquWu"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough  # Allows passing inputs through unchanged in a pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses the model's output into a clean string\n",
        "\n",
        "# Create a retriever to fetch the top 10 most relevant reviews based on a query\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "# The `as_retriever` method converts the database into a retriever.\n",
        "# `k=10` specifies that the retriever should return the top 10 most relevant documents for a query.\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    # Step 1: Retrieves relevant reviews (`context`) and passes the `question` unchanged\n",
        "    | review_prompt_template\n",
        "    # Step 2: Formats the retrieved reviews and the user's question into a structured prompt\n",
        "    | chat_model\n",
        "    # Step 3: Sends the prompt to the Gemini chat model to generate a response\n",
        "    | StrOutputParser()\n",
        "    # Step 4: Parses the model's raw output into a clean string format for easier use\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "tkRCoPB5ro7_",
        "outputId": "61ff729d-dddc-450d-c3d6-276f3681e765"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yes, several patients have complained about communication with the hospital staff.\\n\\nHere are the details:\\n\\n*   **Dennis Fitzgerald (review_id: 43)** complained that the hospital staff lacked proper communication among themselves, which led to confusion about his treatment plan.\\n*   **Jessica Mays (review_id: 773)** encountered communication issues during her stay, noting that the medical staff seemed disorganized, leading to confusion about her treatment plan.\\n*   **Makayla Reynolds (review_id: 707)** experienced issues with the nursing staff's communication, specifically a lack of coordination, which caused confusion about her medication schedule and treatment plan.\\n*   **Eric Duncan (review_id: 977)** was disappointed with the lack of communication between different departments in the hospital, which resulted in confusion about his treatment plan and discharge instructions.\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FG3EW1-laiD"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKaKX6pglaVb"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,  # Template for formatting prompts with dynamic variables\n",
        "    SystemMessagePromptTemplate,  # Represents a system-level instruction to the model\n",
        "    HumanMessagePromptTemplate,  # Represents a human-level input for the model\n",
        "    ChatPromptTemplate,  # Combines multiple prompt components into a unified chat prompt\n",
        ")\n",
        "\n",
        "# Define the system prompt template as a string with placeholders for dynamic content\n",
        "review_template_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "# Create a system-level message prompt template for the chatbot\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"],  # Placeholder for the \"context\" (e.g., patient reviews)\n",
        "        template=review_template_str,  # The instructions and structure of the system prompt\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create a human-level message prompt template for user input\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"],  # Placeholder for the \"question\" to be answered\n",
        "        template=\"{question}\",  # A simple template where the \"question\" is dynamically inserted\n",
        "    )\n",
        ")\n",
        "\n",
        "# Combine the system and human prompts into a list of messages\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# Create a chat prompt template that integrates the system and human prompts\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],  # Define the expected inputs for the template\n",
        "    messages=messages,  # Combine the individual prompt components (system and human)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PFV7mkcjB5W"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CTUzRoNlvDY"
      },
      "outputs": [],
      "source": [
        "# Importing required modules and classes\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,  # Template for structuring prompts\n",
        "    SystemMessagePromptTemplate,  # System-level instructions for the model\n",
        "    HumanMessagePromptTemplate,  # Human input instructions for the model\n",
        "    ChatPromptTemplate,  # Combines system and human prompts into a single chat prompt\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses the model's output into a clean string\n",
        "from langchain_community.vectorstores import Chroma  # Vector database for efficient similarity searches\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Converts text to embeddings using Google's API\n",
        "from langchain_core.runnables import RunnablePassthrough  # Allows passing inputs through unchanged in a pipeline\n",
        "\n",
        "# Path to the persistent Chroma vector database\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process.\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches.\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n",
        "\n",
        "# Loop through the documents in batches to avoid hitting the API's rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    # Get the current batch of documents.\n",
        "    batch_docs = reviews[i:i + batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num}/{num_batches}...\")\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first batch, create a new Chroma vector database.\n",
        "        # The `from_documents` method handles the entire process of embedding and storing the data.\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,  # Pass the list of Document objects that need to be embedded.\n",
        "            embedding=embedding_function,\n",
        "            # Specify the directory on the disk where the vector database will be saved.\n",
        "            # This makes the database persistent, so we can load it directly in the future.\n",
        "            persist_directory=REVIEWS_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        # For subsequent batches, add the documents to the existing database.\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 60 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUxFafojUfL6"
      },
      "outputs": [],
      "source": [
        "reviews_vector_db = Chroma(persist_directory=REVIEWS_CHROMA_PATH, embedding_function=embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV6lQyYDl7WD"
      },
      "outputs": [],
      "source": [
        "# Create a retriever to fetch the top 10 most relevant reviews based on a query\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "# The `as_retriever` method converts the database into a retriever.\n",
        "# `k=10` specifies that the retriever should return the top 10 most relevant documents for a query.\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    # Step 1: Retrieves relevant reviews (`context`) and passes the `question` unchanged\n",
        "    | review_prompt_template\n",
        "    # Step 2: Formats the retrieved reviews and the user's question into a structured prompt\n",
        "    | chat_model\n",
        "    # Step 3: Sends the prompt to the OpenAI chat model to generate a response\n",
        "    | StrOutputParser()\n",
        "    # Step 4: Parses the model's raw output into a clean string format for easier use\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSDO_5aymCIr"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_-TawemeDp"
      },
      "source": [
        "# Adding a UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL-EnjFRmdrb",
        "outputId": "467372de-644b-4541-8cae-5b05b1510bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.21)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnkjE9zXmgqT"
      },
      "outputs": [],
      "source": [
        "def respond_to_user_question(question: str, history: list) -> str:\n",
        "    \"\"\"\n",
        "    Respond to a user's question using the review_chain.\n",
        "    \"\"\"\n",
        "    return review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "Po1hiTd6mko-",
        "outputId": "d81d6f36-2d6e-4abd-cbc7-0c8fdf78b3e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yes, several patients have complained about communication with the hospital staff:\\n\\n*   **Dennis Fitzgerald** (review_id: 43) at Shea LLC stated that the hospital staff lacked proper communication among themselves, which led to confusion about his treatment plan. He emphasized that clear and cohesive communication is essential for patient care and overall satisfaction.\\n*   **Jessica Mays** (review_id: 773) at Rush, Owens and Johnson encountered communication issues during her stay, noting that the medical staff seemed disorganized, which led to confusion about her treatment plan.\\n*   **Makayla Reynolds** (review_id: 707) at Brown-Golden experienced issues with the nursing staff's communication, specifically a lack of coordination, which caused confusion about her medication schedule and treatment plan.\\n*   **Eric Duncan** (review_id: 977) at Richardson-Powell was disappointed with the lack of communication between different departments in the hospital, which led to confusion about his treatment plan and discharge instructions.\""
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "respond_to_user_question(\"Has anyone complained about communication with the hospital staff?\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3vBIk8T_mkln",
        "outputId": "588c4e97-b78f-4514-a4df-6c3b95da828c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://60d74814b5e379ed33.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://60d74814b5e379ed33.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 3047, in _generate\n",
            "    response: GenerateContentResponse = self.client.models.generate_content(\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/models.py\", line 5215, in generate_content\n",
            "    response = self._generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/models.py\", line 3997, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1386, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1220, in _request\n",
            "    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 477, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 378, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 480, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1199, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 18.706117603s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '18s'}]}}\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1696, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 882, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 553, in __wrapper\n",
            "    return await submit_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 943, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3877995097.py\", line 5, in respond_to_user_question\n",
            "    return review_chain.invoke(question)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3151, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 2535, in invoke\n",
            "    return super().invoke(input, config, stop=stop, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 3051, in _generate\n",
            "    _handle_client_error(e, request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 145, in _handle_client_error\n",
            "    raise ChatGoogleGenerativeAIError(msg) from e\n",
            "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 18.706117603s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '18s'}]}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 3047, in _generate\n",
            "    response: GenerateContentResponse = self.client.models.generate_content(\n",
            "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/models.py\", line 5215, in generate_content\n",
            "    response = self._generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/models.py\", line 3997, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1386, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1220, in _request\n",
            "    return retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 477, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 378, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 480, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1199, in _request_once\n",
            "    errors.APIError.raise_for_response(response)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\", line 121, in raise_for_response\n",
            "    cls.raise_error(response.status_code, response_json, response)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\", line 146, in raise_error\n",
            "    raise ClientError(status_code, response_json, response)\n",
            "google.genai.errors.ClientError: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 27.314146589s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}}\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1696, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 882, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 553, in __wrapper\n",
            "    return await submit_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 943, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3877995097.py\", line 5, in respond_to_user_question\n",
            "    return review_chain.invoke(question)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3151, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 2535, in invoke\n",
            "    return super().invoke(input, config, stop=stop, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n",
            "    self.generate_prompt(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n",
            "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 3051, in _generate\n",
            "    _handle_client_error(e, request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\", line 145, in _handle_client_error\n",
            "    raise ChatGoogleGenerativeAIError(msg) from e\n",
            "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 27.314146589s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '27s'}]}}\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "# Create the Gradio ChatInterface\n",
        "interface = gr.ChatInterface(fn=respond_to_user_question, title=\"Review Helper Bot\")\n",
        "\n",
        "# Launch the Gradio app\n",
        "interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl7W3EFfuhvT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsUpDckHug1H"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Zb_TKiNkaA"
      },
      "source": [
        "# Your goal is to get one incorrect (out of context) answer, or 5 really amazing answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0GVCD-jNwra"
      },
      "source": [
        "Submit screenshots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LziiVwG0A9YZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
